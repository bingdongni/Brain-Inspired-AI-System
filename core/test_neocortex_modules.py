"""
æ–°çš®å±‚æ¨¡æ‹Ÿå™¨æ ¸å¿ƒæ¨¡å—æµ‹è¯•è„šæœ¬
============================

æµ‹è¯•æ‰€æœ‰æ–°çš®å±‚æ¨¡æ‹Ÿå™¨çš„æ ¸å¿ƒç»„ä»¶ï¼š
1. åˆ†å±‚æŠ½è±¡æœºåˆ¶æµ‹è¯•
2. ä¸“ä¸šå¤„ç†æ¨¡å—æµ‹è¯•  
3. çŸ¥è¯†æŠ½è±¡ç®—æ³•æµ‹è¯•
4. ç¨€ç–æ¿€æ´»å’Œæƒé‡å·©å›ºæµ‹è¯•
5. æ–°çš®å±‚æ¶æ„é›†æˆæµ‹è¯•

è¿è¡Œæ–¹å¼ï¼š
python test_neocortex_modules.py
"""

import torch
import torch.nn as nn
import numpy as np
import sys
import os

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

# å¯¼å…¥æ–°çš®å±‚æ¨¡æ‹Ÿå™¨æ ¸å¿ƒæ¨¡å—
from core import (
    # åˆ†å±‚æŠ½è±¡æœºåˆ¶
    HierarchicalLayer, LayerConfig, ProcessingHierarchy, LayerType, ProcessingMode,
    create_visual_hierarchy, create_auditory_hierarchy,
    
    # ä¸“ä¸šå¤„ç†æ¨¡å—
    PredictionModule, AttentionModule, DecisionModule, CrossModalModule,
    PredictionType, AttentionType, DecisionMode, ProcessingConfig,
    create_prediction_module, create_attention_module, 
    create_decision_module, create_crossmodal_module,
    
    # çŸ¥è¯†æŠ½è±¡ç®—æ³•
    AbstractionEngine, ConceptUnit, SemanticAbstraction,
    ConceptConfig, ConceptType, AbstractionLevel,
    create_abstraction_engine, create_concept_units,
    
    # ç¨€ç–æ¿€æ´»å’Œæƒé‡å·©å›º
    ConsolidationEngine, SparseActivation, WeightConsolidation, EngramCell,
    ConsolidationConfig, EngramConfig, CellType, MemoryState,
    create_consolidation_engine,
    
    # æ–°çš®å±‚æ¶æ„
    NeocortexSimulator, TONN, ModularNeocortex,
    NeocortexConfig, ArchitectureType
)


def test_hierarchical_layers():
    """æµ‹è¯•åˆ†å±‚æŠ½è±¡æœºåˆ¶"""
    print("=== æµ‹è¯•åˆ†å±‚æŠ½è±¡æœºåˆ¶ ===")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºè§†è§‰å±‚æ¬¡é…ç½®
    visual_configs = create_visual_hierarchy(input_channels=3)
    print(f"è§†è§‰å±‚æ¬¡é…ç½®: {len(visual_configs)}å±‚")
    for i, config in enumerate(visual_configs):
        print(f"  å±‚{i+1}: {config.layer_type.value} - {config.input_channels}->{config.output_channels}")
    
    # åˆ›å»ºå¤„ç†å±‚æ¬¡
    visual_hierarchy = ProcessingHierarchy(visual_configs).to(device)
    print(f"âœ“ åˆ›å»ºè§†è§‰å¤„ç†å±‚æ¬¡æˆåŠŸ")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    test_input = torch.randn(2, 3, 224, 224).to(device)
    outputs = visual_hierarchy(test_input)
    
    print(f"âœ“ è§†è§‰å±‚æ¬¡å‰å‘ä¼ æ’­æˆåŠŸ")
    print(f"  è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    print(f"  æœ€ç»ˆè¾“å‡ºå½¢çŠ¶: {outputs['final_output'].shape}")
    print(f"  å±‚æ¬¡æ•°é‡: {len(outputs['layer_outputs'])}")
    
    # åˆ›å»ºå¬è§‰å±‚æ¬¡é…ç½®
    auditory_configs = create_auditory_hierarchy(input_channels=1)
    print(f"âœ“ å¬è§‰å±‚æ¬¡é…ç½®: {len(auditory_configs)}å±‚")
    
    return True


def test_processing_modules():
    """æµ‹è¯•ä¸“ä¸šå¤„ç†æ¨¡å—"""
    print("\n=== æµ‹è¯•ä¸“ä¸šå¤„ç†æ¨¡å— ===")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # æµ‹è¯•é¢„æµ‹æ¨¡å—
    prediction_module = create_prediction_module(128).to(device)
    test_input = torch.randn(4, 10, 128).to(device)  # batch, seq_len, dim
    pred_output = prediction_module(test_input)
    
    print(f"âœ“ é¢„æµ‹æ¨¡å—æµ‹è¯•æˆåŠŸ")
    print(f"  è¾“å…¥: {test_input.shape}")
    print(f"  é¢„æµ‹è¾“å‡º: {pred_output['predictions'].shape}")
    print(f"  é¢„æµ‹è¯¯å·®: {pred_output['prediction_errors'].shape}")
    print(f"  å¹³å‡ç½®ä¿¡åº¦: {pred_output['confidence'].mean().item():.3f}")
    
    # æµ‹è¯•æ³¨æ„æ¨¡å—
    attention_module = create_attention_module(256).to(device)
    test_features = torch.randn(4, 256, 14, 14).to(device)
    attention_output = attention_module(test_features)
    
    print(f"âœ“ æ³¨æ„æ¨¡å—æµ‹è¯•æˆåŠŸ")
    print(f"  è¾“å…¥ç‰¹å¾: {test_features.shape}")
    print(f"  æ³¨æ„ç‰¹å¾: {attention_output['attended_features'].shape}")
    print(f"  ç©ºé—´æ³¨æ„: {attention_output['spatial_attention'].shape}")
    
    # æµ‹è¯•å†³ç­–æ¨¡å—
    decision_module = create_decision_module(512, 2).to(device)
    test_responses = torch.randn(4, 512).to(device)
    decision_output = decision_module(test_responses)
    
    print(f"âœ“ å†³ç­–æ¨¡å—æµ‹è¯•æˆåŠŸ")
    print(f"  ç¥ç»å…ƒå“åº”: {test_responses.shape}")
    print(f"  å†³ç­–è¾“å‡º: {decision_output['decision'].shape}")
    print(f"  ç½®ä¿¡åº¦: {decision_output['confidence'].mean().item():.3f}")
    
    # æµ‹è¯•è·¨æ¨¡æ€æ¨¡å—
    crossmodal_module = create_crossmodal_module(256, 128).to(device)
    visual_input = torch.randn(4, 256).to(device)
    language_input = torch.randn(4, 256).to(device)
    crossmodal_output = crossmodal_module(visual_input, language_input)
    
    print(f"âœ“ è·¨æ¨¡æ€æ¨¡å—æµ‹è¯•æˆåŠŸ")
    print(f"  è§†è§‰è¾“å…¥: {visual_input.shape}")
    print(f"  è¯­è¨€è¾“å…¥: {language_input.shape}")
    print(f"  æ¦‚å¿µè¡¨ç¤º: {crossmodal_output['concept_representation'].shape}")
    print(f"  æŠ½è±¡è¡¨ç¤º: {crossmodal_output['abstract_representation'].shape}")
    print(f"  è·¨æ¨¡æ€ä¸€è‡´æ€§: {crossmodal_output['cross_modal_consistency'].mean().item():.3f}")
    
    return True


def test_abstraction_algorithms():
    """æµ‹è¯•çŸ¥è¯†æŠ½è±¡ç®—æ³•"""
    print("\n=== æµ‹è¯•çŸ¥è¯†æŠ½è±¡ç®—æ³• ===")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºæŠ½è±¡å¼•æ“
    abstraction_engine = create_abstraction_engine(256, 20).to(device)
    print(f"âœ“ åˆ›å»ºæŠ½è±¡å¼•æ“æˆåŠŸ")
    
    # æµ‹è¯•æ¦‚å¿µå•å…ƒ
    concept_configs = [
        ConceptConfig(
            concept_id=f"concept_{i}",
            concept_type=ConceptType.OBJECT,
            abstraction_level=AbstractionLevel.CONCEPTUAL
        )
        for i in range(5)
    ]
    
    concept_units = create_concept_units(256, 5)
    print(f"âœ“ åˆ›å»º{len(concept_units)}ä¸ªæ¦‚å¿µå•å…ƒ")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    test_features = torch.randn(4, 256).to(device)
    results = abstraction_engine(test_features)
    
    print(f"âœ“ æŠ½è±¡å¼•æ“æµ‹è¯•æˆåŠŸ")
    print(f"  è¾“å…¥ç‰¹å¾: {test_features.shape}")
    print(f"  æ¿€æ´»æ¦‚å¿µæ•°é‡: {results['abstraction_summary']['num_active_concepts']}")
    print(f"  æŠ½è±¡å±‚æ¬¡: {results['abstraction_summary']['abstraction_level']:.3f}")
    print(f"  æ³›åŒ–åˆ†æ•°: {results['abstraction_summary']['generalization_score']:.3f}")
    print(f"  æœ€ç»ˆæŠ½è±¡è¡¨ç¤º: {results['final_abstraction']['integrated_representation'].shape}")
    print(f"  æŠ½è±¡è´¨é‡: {results['final_abstraction']['abstraction_quality'].mean().item():.3f}")
    
    # æµ‹è¯•è¯­ä¹‰æŠ½è±¡ç»„ä»¶
    semantic_abstraction = SemanticAbstraction(256, 4)
    semantic_results = semantic_abstraction(test_features)
    
    print(f"âœ“ è¯­ä¹‰æŠ½è±¡ç»„ä»¶æµ‹è¯•æˆåŠŸ")
    print(f"  æŠ½è±¡å±‚æ¬¡: {len(semantic_results['abstraction_levels'])}")
    print(f"  æ¦‚å¿µèšç±»æ•°é‡: {semantic_results['concept_clusters']['num_active_clusters']}")
    print(f"  è¯­ä¹‰å…³ç³»æ•°é‡: {semantic_results['semantic_relations']['num_relations']}")
    
    return True


def test_sparse_activation_consolidation():
    """æµ‹è¯•ç¨€ç–æ¿€æ´»å’Œæƒé‡å·©å›º"""
    print("\n=== æµ‹è¯•ç¨€ç–æ¿€æ´»å’Œæƒé‡å·©å›º ===")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºå·©å›ºå¼•æ“
    consolidation_engine = create_consolidation_engine(256, 50).to(device)
    print(f"âœ“ åˆ›å»ºå·©å›ºå¼•æ“æˆåŠŸ")
    
    # æµ‹è¯•è¾“å…¥
    test_features = torch.randn(4, 256).to(device)
    results = consolidation_engine(test_features)
    
    print(f"âœ“ å·©å›ºå¼•æ“æµ‹è¯•æˆåŠŸ")
    print(f"  è¾“å…¥ç‰¹å¾: {test_features.shape}")
    print(f"  ç¨€ç–æ•ˆç‡: {results['consolidation_summary']['sparse_efficiency']:.3f}")
    print(f"  æ¿€æ´»å°è®°ç»†èƒ: {results['consolidation_summary']['active_engram_cells']}")
    print(f"  å½¢æˆå°è®°æ•°é‡: {results['consolidation_summary']['formed_engrams']}")
    print(f"  å¹³å‡å°è®°å¼ºåº¦: {results['consolidation_summary']['avg_engram_strength']:.3f}")
    print(f"  å·©å›ºè´¨é‡: {results['consolidation_summary']['consolidation_quality']:.3f}")
    print(f"  è®°å¿†å¥åº·åº¦: {results['memory_state']['memory_health']:.3f}")
    
    # æµ‹è¯•ç¨€ç–æ¿€æ´»ç»„ä»¶
    sparse_activation = SparseActivation(ConsolidationConfig(), 256).to(device)
    sparse_results = sparse_activation(test_features)
    
    print(f"âœ“ ç¨€ç–æ¿€æ´»ç»„ä»¶æµ‹è¯•æˆåŠŸ")
    print(f"  å®é™…ç¨€ç–æ€§: {sparse_results['actual_sparsity'].mean().item():.3f}")
    print(f"  ç¨€ç–æ•ˆç‡: {sparse_results['sparse_efficiency'].mean().item():.3f}")
    
    # æµ‹è¯•æƒé‡å·©å›ºç»„ä»¶
    weight_consolidation = WeightConsolidation(
        ConsolidationConfig(), (256, 256)
    ).to(device)
    
    # åˆ›å»ºæµ‹è¯•æƒé‡
    current_weights = torch.randn(256, 256) * 0.1
    weight_changes = torch.randn(256, 256) * 0.01
    
    consolidation_results = weight_consolidation(current_weights, weight_changes)
    
    print(f"âœ“ æƒé‡å·©å›ºç»„ä»¶æµ‹è¯•æˆåŠŸ")
    print(f"  å½“å‰æƒé‡å½¢çŠ¶: {current_weights.shape}")
    print(f"  å·©å›ºè´¨é‡: {consolidation_results['consolidation_quality'].item():.3f}")
    print(f"  æƒé‡ç¨³å®šæ€§: {consolidation_results['weight_stability'].item():.3f}")
    print(f"  è®°å¿†å¼ºåº¦: {consolidation_results['memory_strength'].item():.3f}")
    
    return True


def test_neocortex_architecture():
    """æµ‹è¯•æ–°çš®å±‚æ¶æ„"""
    print("\n=== æµ‹è¯•æ–°çš®å±‚æ¶æ„ ===")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºæ–°çš®å±‚é…ç½®
    config = NeocortexConfig(
        architecture_type=ArchitectureType.TONN,
        input_dim=512,
        hidden_dim=256,
        output_dim=128,
        prediction_enabled=True,
        attention_enabled=True,
        abstraction_enabled=True,
        consolidation_enabled=True,
        decision_enabled=True,
        num_concepts=30,
        num_engram_cells=60
    )
    
    print(f"âœ“ åˆ›å»ºæ–°çš®å±‚é…ç½®")
    print(f"  æ¶æ„ç±»å‹: {config.architecture_type.value}")
    print(f"  è¾“å…¥ç»´åº¦: {config.input_dim}")
    print(f"  éšè—ç»´åº¦: {config.hidden_dim}")
    
    # åˆ›å»ºæ–°çš®å±‚æ¨¡æ‹Ÿå™¨
    neocortex_simulator = NeocortexSimulator(config).to(device)
    print(f"âœ“ åˆ›å»ºæ–°çš®å±‚æ¨¡æ‹Ÿå™¨æˆåŠŸ")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    test_inputs = {
        'visual': torch.randn(2, 512).to(device),
        'multimodal': torch.randn(2, 512).to(device)
    }
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    results = neocortex_simulator(test_inputs)
    
    print(f"âœ“ æ–°çš®å±‚æ¨¡æ‹Ÿå™¨æµ‹è¯•æˆåŠŸ")
    print(f"  è¾“å…¥å½¢çŠ¶: {test_inputs['visual'].shape}")
    print(f"  æ€»å¤„ç†é˜¶æ®µ: {len(results['stage_outputs'])}")
    print(f"  æœ€ç»ˆè¾“å‡ºå½¢çŠ¶: {results['final_output'].shape}")
    print(f"  æ¶æ„ç±»å‹: {results['summary']['architecture_type']}")
    print(f"  å¤„ç†æ•ˆç‡: {results['summary']['processing_efficiency']:.3f}")
    
    # æ˜¾ç¤ºå„é˜¶æ®µä¿¡æ¯
    print(f"\nå¤„ç†é˜¶æ®µè¯¦æƒ…:")
    for stage, info in results['stage_info'].items():
        print(f"  {stage.value}: {info}")
    
    # æ€§èƒ½æŒ‡æ ‡
    perf_metrics = results['performance_metrics']
    print(f"\næ€§èƒ½æŒ‡æ ‡:")
    print(f"  å¤„ç†æ•ˆç‡: {perf_metrics['processing_efficiency']:.3f}")
    print(f"  è¾“å‡ºè´¨é‡: {perf_metrics['output_quality']:.3f}")
    print(f"  èµ„æºä½¿ç”¨: {perf_metrics['resource_usage']}")
    print(f"  æ•´ä½“æ•ˆç‡: {perf_metrics['overall_efficiency']:.3f}")
    
    # æµ‹è¯•TONN
    tonn = TONN(config).to(device)
    tonn_results = tonn(test_inputs)
    
    print(f"\nâœ“ TONNæµ‹è¯•æˆåŠŸ")
    print(f"  TONNè¾“å‡ºå½¢çŠ¶: {tonn_results['final_output'].shape}")
    
    # æµ‹è¯•æ¨¡å—åŒ–æ¶æ„
    modular_neocortex = ModularNeocortex(config).to(device)
    modular_neocortex.configure_modules(['hierarchical', 'attention', 'decision'])
    modular_results = modular_neocortex(test_inputs)
    
    print(f"âœ“ æ¨¡å—åŒ–æ¶æ„æµ‹è¯•æˆåŠŸ")
    print(f"  æ¨¡å—åŒ–è¾“å‡ºå½¢çŠ¶: {modular_results['final_output'].shape}")
    print(f"  æ´»è·ƒæ¨¡å—: {list(modular_results['module_results'].keys())}")
    
    return True


def performance_benchmark():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\n=== æ€§èƒ½åŸºå‡†æµ‹è¯• ===")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # å¯¼å…¥æ—¶é—´æ¨¡å—
    import time
    
    # åˆ›å»ºé…ç½®
    config = NeocortexConfig(
        architecture_type=ArchitectureType.TONN,
        input_dim=512,
        hidden_dim=256,
        output_dim=128,
        prediction_enabled=True,
        attention_enabled=True,
        abstraction_enabled=True,
        consolidation_enabled=True,
        decision_enabled=True
    )
    
    # åˆ›å»ºæ¨¡æ‹Ÿå™¨
    neocortex = NeocortexSimulator(config).to(device)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    batch_sizes = [1, 4, 8, 16]
    test_inputs = {
        'visual': torch.randn(1, 512).to(device),
        'multimodal': torch.randn(1, 512).to(device)
    }
    
    print("æ‰¹æ¬¡å¤§å° | å‰å‘ä¼ æ’­æ—¶é—´ | å†…å­˜ä½¿ç”¨(MB)")
    print("-" * 45)
    
    for batch_size in batch_sizes:
        # åˆ›å»ºå¯¹åº”æ‰¹æ¬¡å¤§å°çš„è¾“å…¥
        batch_inputs = {
            'visual': torch.randn(batch_size, 512).to(device),
            'multimodal': torch.randn(batch_size, 512).to(device)
        }
        
        # æ¸…ç©ºGPUç¼“å­˜
        if device.type == 'cuda':
            torch.cuda.empty_cache()
        
        # æµ‹é‡å‰å‘ä¼ æ’­æ—¶é—´
        start_time = time.time()
        with torch.no_grad():
            _ = neocortex(batch_inputs)
        forward_time = time.time() - start_time
        
        # æµ‹é‡å†…å­˜ä½¿ç”¨
        if device.type == 'cuda':
            memory_used = torch.cuda.memory_allocated(device) / 1024 / 1024
        else:
            memory_used = 0
        
        print(f"{batch_size:^8} | {forward_time:^12.4f} | {memory_used:^12.1f}")
    
    print(f"\nâœ“ æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ§  æ–°çš®å±‚æ¨¡æ‹Ÿå™¨æ ¸å¿ƒæ¨¡å—æµ‹è¯•")
    print("=" * 50)
    
    try:
        # åŸºç¡€æ¨¡å—æµ‹è¯•
        test_hierarchical_layers()
        test_processing_modules()
        test_abstraction_algorithms()
        test_sparse_activation_consolidation()
        test_neocortex_architecture()
        
        # æ€§èƒ½æµ‹è¯•
        performance_benchmark()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ–°çš®å±‚æ¨¡æ‹Ÿå™¨æ ¸å¿ƒæ¨¡å—å·¥ä½œæ­£å¸¸ã€‚")
        print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
        print("  âœ“ åˆ†å±‚æŠ½è±¡æœºåˆ¶ - æ­£å¸¸")
        print("  âœ“ ä¸“ä¸šå¤„ç†æ¨¡å— - æ­£å¸¸")
        print("  âœ“ çŸ¥è¯†æŠ½è±¡ç®—æ³• - æ­£å¸¸")
        print("  âœ“ ç¨€ç–æ¿€æ´»å’Œæƒé‡å·©å›º - æ­£å¸¸")
        print("  âœ“ æ–°çš®å±‚æ¶æ„ - æ­£å¸¸")
        print("\nğŸš€ æ–°çš®å±‚æ¨¡æ‹Ÿå™¨å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥è¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå¼€å‘ï¼")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
