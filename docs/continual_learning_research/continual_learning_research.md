# 终身学习的神经科学原理与计算方法:机制、算法与前沿进展

## 导言:为何终身学习,以及本报告的研究路径

在动态开放环境中,机器学习系统需要不断吸纳新知识、适应新任务,同时保持对旧知识的稳定记忆。然而,标准的顺序训练常常遭遇“灾难性遗忘”:新任务参数的更新会迅速覆盖旧任务的表征,导致性能骤降。持续学习(Continual Learning)旨在构建能够在时间维度上不断学习、同时管理稳定性—可塑性权衡的智能系统,其目标是兼顾“记住旧知识”与“学会新任务”,并在资源受限的现实约束下维持高效与安全[^1]。

终身学习不仅是工程问题,更有深刻的神经科学根基。大脑通过多时间尺度的巩固机制,将短期记忆转化为长期记忆;通过离线回放与价值导向的选择,优化决策与表征;通过可塑性—保护性平衡,既维持已有知识,又为新经验留出空间[^2]。受此启发,人工系统发展出三大策略:正则化/参数约束(如弹性权重巩固,EWC)、回放/生成式重演(如生成式回放)、动态架构扩展(如渐进式网络、参数隔离)。这些策略相互补充,形成多样化的持续学习解决方案。

本报告围绕六个关键问题展开:一是灾难性遗忘的神经机制与人工网络的映射;二是EWC的原理、实现与实践细节;三是生成式回放的计算框架及其与海马—皮层系统的同构关系;四是动态网络扩展技术在不同任务场景中的优势与成本;五是知识迁移与重用的有效方法与评估;六是截至2025年的最新研究进展(类人遗忘、类脑架构、脉冲神经网络的持续学习等)。为保证评估与比较的可操作性,我们同时给出主流基准与指标框架,强调任务增量、领域增量、在线学习等设置下的度量方法[^1]。

叙事结构上,我们由“是什么”(机制与概念)出发,转入“怎么做”(算法与实现),最终讨论“为何重要”(前沿与战略),以期在神经科学证据与计算模型之间搭起相互验证的桥梁[^1][^2]。

---

## 第一部分:灾难性遗忘的神经机制

大脑的时间分层记忆可大致划分为短期巩固(数分钟至数小时)、突触巩固(数小时至数天)、系统巩固(数天至数周乃至数年)三个层次。突触层面,长期增强(LTP)与长期抑制(LTD)以及多种分子与形态学改变共同塑造持久记忆痕迹;系统层面,海马与新皮层通过回放与耦合实现信息的再编码与整合[^3][^4][^5]。在离线阶段,尤其是慢波睡眠与安静休息期,海马尖波涟漪(Sharp-Wave Ripples, SWRs)驱动的重放,配合默认模式网络(Default Mode Network, DMN)的跨区协调,促进价值相关的选择与巩固[^2][^6]。此外,神经科学近期揭示“选择性巩固”与“回忆门控巩固”的机制:并非所有记忆都同等加固,带有行为价值或高频回忆的表征更可能被稳定到长期存储中[^4][^5]。从系统层面看,时间依赖性整合与跨时段巩固共同决定记忆的持久性与可恢复性[^6]。在人工网络上,这些机制可映射为基于重要性的参数保护、基于回放的经验重演、基于价值加权的采样与正则化等设计。

为便于直观对比,下面的映射表概括了关键神经机制与人工持续学习策略之间的关系。

为了具体化上述论证,表1总结了生物机制到人工方法的映射,并讨论其要点与权衡。

表1 神经机制—人工方法映射表

| 生物机制 | 关键现象/时间尺度 | 人工对应策略 | 适用场景 | 风险与权衡 |
|---|---|---|---|---|
| 突触巩固(LTP/LTD、蛋白质合成、形态重塑) | 分钟—小时(突触),天—周(系统)[^3] | 参数重要性加权(EWC)、二次惩罚、学习率调制 | 任务增量、容量有限情形 | 重要性估计偏差会导致保护过度或保护不足;高维下Fisher近似可能不稳[^10][^11] |
| 系统巩固(海马—皮层耦合) | 天—周—年[^5] | 外部重放缓存与分层表征学习 | 类别增量、长期演化知识库 | 存储成本与数据隐私;重放分布偏移风险 |
| 重放(SWRs、价值依赖) | 离线睡眠/静息期,140–200 Hz涟漪[^7] | 经验回放(真实/生成)、价值引导采样 | 强化学习、序列决策 | 低价值记忆被忽略可能引发偏见;生成质量不足造成干扰[^7][^12] |
| 选择性巩固与回忆门控巩固 | 回忆触发与稳定化耦合[^4][^5] | 记忆选择与门控正则化、优先重放 | 资源受限、任务分布不均 | 偏倚强化可能削弱多样性;门控策略需稳健 |
| DMN介导的级联巩固 | 静息网络协同与长距离相关[^2] | 双侧/对称巩固架构(双向巩固) | 长时程记忆整合 | 结构复杂性与同步成本;跨模态对齐挑战[^26] |

如表1所示,神经系统的多尺度巩固与价值依赖选择为人工系统提供了设计蓝图:一方面通过重要性估计与参数保护防止旧知识被覆写;另一方面通过离线回放与选择门控实现高效且有偏见的经验利用。这一“保护—重放—选择”的三元组合,构成了当前持续学习算法的主干。

### 突触与系统巩固的时间分层

在时间维度上,突触巩固通常发生在学习后的数分钟到数小时,涉及受体激活、蛋白质磷酸化、局部转录与蛋白质合成,以及突触形态重构;随后是系统巩固,在海马与新皮层之间通过反复回放与再编码,逐步形成稳定的长期表征,时间跨度可从数天到数年[^3][^5]。这种分层的时间结构为人工系统提供了启示:当无法在单次训练中完全消化新知识时,可通过分阶段的“巩固—重放—整合”策略,逐步稳定表征。

### 价值依赖重放与离线巩固的计算映射

海马—皮层系统不仅重现过往经历,还依据价值进行选择性重放:例如在SWRs期间,通向奖励位置的轨迹被优先再激活,从而在离线阶段强化高价值的策略与表征。这种“模拟—选择”机制可映射为强化学习中的环境模型生成(CA3类比)与价值评估(CA1类比),与Dyna家族的“规划—学习”框架具有天然对应关系[^7][^12]。在人工系统中,价值引导的生成式回放与优先重放策略能够提升样本效率,并在一定程度上缓解遗忘。

---

## 第二部分:弹性权重巩固(EWC)与参数约束家族

弹性权重巩固(Elastic Weight Consolidation, EWC)是目前影响最广的参数约束类方法之一,核心思想是在学习新任务时,对与旧任务相关的关键参数施加二次惩罚,从而在参数空间中形成“弹性”约束,保留旧知识的同时允许对新任务进行适度的可塑性调整[^10][^11]。从贝叶斯视角看,EWC通过Fisher信息矩阵(FIM)近似参数后验分布的对数泰勒展开,从而在损失函数中加入对重要参数的加权约束;工程上通常采用FIM的对角近似以降低复杂度,使计算代价与参数规模线性相关。

为便于理解,我们先以数学形式概述其目标函数,然后讨论实现细节与工程注意事项。

表2 EWC目标函数与参数说明表

| 项目 | 公式/符号 | 含义 |
|---|---|---|
| 总损失 | L(θ) = L_B(θ) + Σ_i (λ/2) F_i (θ_i − θ*_{A,i})^2 | L_B为新任务损失;第二项为EWC二次惩罚[^10] |
| 权重参数 | θ_i | 网络第i个参数 |
| 任务A参数 | θ*_{A,i} | 旧任务训练后参数 |
| Fisher对角元 | F_i | 参数重要性估计,来源于FIM对角近似 |
| 超参数 | λ | 控制旧/新任务权衡,决定保护强度 |
| 计算复杂度 | O(∑参数·样本) | 线性于参数与样本数,避免矩阵求逆[^10] |

EWC在多个任务序列上显示出显著优势,尤其是在置换MNIST与Atari游戏序列任务中,能够保留更高的旧任务性能并维持较稳定的信号噪声比(SNR),相较于L2正则与朴素SGD有明确改进[^10]。然而,EWC也存在实践挑战:在高维与多任务场景下,Fisher对角近似可能低估参数相关性,导致保护策略偏弱或偏强;二次惩罚的曲率选择与λ调度会显著影响结果。此外,随着任务数量增长,累计约束可能削弱模型容量,需要与回放或架构扩展策略协同使用[^11]。

表3 参数约束类方法比较表

| 方法 | 机制 | 优点 | 局限 | 典型场景 |
|---|---|---|---|---|
| EWC[^10] | Fisher对角近似的二次惩罚 | 简洁高效、线性复杂度、有生物启发 | 重要性估计偏差、曲率与λ敏感、长期容量压力 | 任务增量、资源受限 |
| L2正则 | 参数值收缩 | 易实现、通用 | 不区分参数重要性,保护效果弱 | 基线、与其他方法结合 |
| MAS[^1] | 无需标签的参数重要性 | 实用、适合无监督场景 | 重要性度量可能不稳、规模大时内存压力 | 工业部署、半监督 |

在工程实践中,EWC的关键环节包括:合理估计Fisher(采样数、稳态覆盖)、稳健计算(数值稳定、避免上溢/下溢)、λ调度(任务相似度自适应),以及与重放/蒸馏的协同以降低过约束风险[^10][^11]。

### Fisher信息矩阵与贝叶斯近似

Fisher信息矩阵衡量参数扰动对似然的影响,是后验不确定性的代理。EWC将FIM对角元作为参数重要性权重,构成二次惩罚项。对角近似将复杂度降至线性,但也忽略了参数间的相关性,导致某些联合敏感的参数组未被充分保护。在多任务相似度较高时,这种近似尚可接受;在高度异构任务序列下,可能需要块对角近似或更精细的估计策略,或采用与回放、架构扩展的混合策略缓解局限[^10][^1]。

### 超参数敏感性与实践策略

EWC的性能高度依赖λ与Fisher估计的规模与质量。过强的λ会压制新任务学习,造成可塑性不足;过弱则无法有效防遗忘。建议使用任务相似度感知的自适应λ调度,或在训练初期进行小规模Fisher探测以校准权重。实践中,EWC与知识蒸馏或少量真实样本回放结合,能有效缓解“过度保护”,提高新任务收敛速度与整体稳定性[^10][^1]。

---

## 第三部分:生成式回放(Rehearsal/Replay)机制

回放在生物系统中至关重要:在安静休息与慢波睡眠期,海马通过SWRs触发对过往经历的重放,以支持系统巩固与策略优化[^12]。在人工持续学习中,回放通常通过两类方式实现:一是直接存储少量真实样本的经验回放(rehearsal),二是利用生成模型合成伪样本的生成式回放(regenerative rehearsal)。生成式回放可以在不存储原始数据的前提下重演经验,缓解数据隐私与存储成本问题,但其效果高度依赖生成质量与任务分布覆盖[^13][^16]。

在框架上,可将生成式回放视为“生成—选择—整合”的三层过程:生成器产生候选样本,选择机制依据价值或不确定性进行优先采样,整合模块通过蒸馏或联合训练将新旧知识对齐。为避免生成分布偏移,常采用分布对齐与去偏技术,并与记忆压缩结合以提升存储与采样效率[^13][^14]。在强化学习视角下,海马体CA3的“模拟”与CA1的“选择”可映射为环境模型生成与价值评估,对应Dyna架构;这种映射为价值依赖的生成式回放提供了自然的计算语义[^7]。

表4 回放类型与生成策略比较表

| 回放类型 | 生成/存储方式 | 存储成本 | 分布对齐 | 典型应用 | 风险 |
|---|---|---|---|---|---|
| 真实样本回放 | 小规模缓存 | 中—高 | 天然对齐 | 类别增量、经验回放 | 隐私、存储、代表性不足 |
| 生成式回放 | 生成器合成 | 低—中 | 需对齐与去偏 | 数据敏感场景、序列建模 | 生成质量不足引发干扰[^13][^16] |
| 压缩回放(MRDC) | 压缩后缓存 | 低 | 需重建校准 | 大规模视觉、长期学习 | 重建误差与偏差[^14] |
| 双侧/对称巩固 | 双向巩固机制 | 中 | 结构内对齐 | 跨阶段整合 | 架构复杂与同步成本[^26] |

在长期记忆巩固中,时间依赖机制与跨时段整合尤为关键。近期研究强调“回忆门控巩固”与“选择性巩固”的耦合效应:只有被反复回忆或具有较高行为价值的表征才会进入稳定巩固通道;这一机制与价值依赖回放共同决定哪些记忆能够跨越时间保持下来[^5][^6]。在人工系统中,这意味着需要设计可解释的选择门控与优先级策略,避免对低价值或噪声的过度巩固,从而维持模型的广谱记忆能力。

### 压缩回放与长期巩固

为降低存储成本,压缩回放(Memory Replay with Data Compression, MRDC)通过对样本进行压缩编码与重建,在有限的缓存容量下最大化覆盖度。关键在于平衡压缩率与重建误差,避免因过度压缩导致分布偏移,从而在回放时引入偏差。实践上,建议在压缩后进行质量评估与分布校准,并结合蒸馏损失稳定新旧表征的对齐[^14]。

---

## 第四部分:动态网络扩展技术

参数约束与回放策略管理的是“共享参数”的稳定性;另一条主线是动态架构扩展,通过为新任务分配新的参数容量来避免干扰。代表性方法包括渐进式网络(Progressive Neural Networks, PNN)、剪枝/扩容结合的渐进学习,以及参数隔离(如PackNet、Piggyback)等。PNN在每个新任务到来时扩展新的列/模块,保留旧任务列并冻结相应参数,同时通过横向连接促进跨任务的知识共享;剪枝与课程学习则用于控制容量增长与稀疏化,以提高推理效率[^15][^16][^18]。

表5 动态扩展方法比较表

| 方法 | 容量增长 | 参数共享 | 遗忘控制 | 存储/推理成本 | 适用场景 |
|---|---|---|---|---|---|
| PNN[^15][^18] | 逐任务扩容 | 横向连接、列间共享 | 强(冻结旧列) | 存储增长线性于任务数 | 任务增量、强化学习 |
| 渐进学习[^16] | 课程+扩容+剪枝 | 视剪枝策略共享 | 中—强 | 剪枝后推理高效 | 结构化任务、视觉 |
| 参数隔离[^1] | 掩码/分配固定参数 | 低共享(隔离) | 强 | 存储中等、推理低 | 高度异构任务、强隔离需求 |

在多任务强化学习(如Atari)中,PNN通过冻结旧列并扩展新列有效缓解遗忘,并能保留跨任务的可迁移表征;然而其存储与推理成本随任务数增长而线性提升,需结合剪枝与稀疏化策略控制规模[^15][^18]。在视觉分类等任务中,渐进学习的“课程—扩容—剪枝”三段式流程能够在不显著增加推理负担的前提下提升持续性能,但对剪枝策略与课程设计较为敏感[^16]。

### PNN的横向连接与任务间共享

PNN的横向连接旨在在保持隔离的同时实现适度的知识迁移。对于相似任务,这种共享能提升参数利用率与泛化;对于异构任务,过度共享可能引入干扰。因此,实践中常配合门控或掩码机制,动态调节共享强度,确保隔离与迁移的平衡[^15][^18]。

---

## 第五部分:知识迁移与重用

持续学习并非孤立的“防遗忘”,更关键在于高效迁移与重用既有知识,以降低新任务学习成本并提升整体性能。蒸馏家族(如Learning Without Forgetting, LwF)通过保持旧模型的输出分布来约束新模型,使得新任务学习不会破坏旧知识;配合少量真实样本或生成式回放,能够在低风险下引入新任务信息。课程学习(curriculum)通过合理的任务顺序与渐进式训练,降低学习难度与灾难性遗忘的风险;在架构扩展中,课程策略与剪枝配合,有助于形成稀疏而高效的跨任务表征[^1][^16]。

从系统层面看,迁移与重用的度量不应仅以准确率或回报衡量,还需关注前向/后向迁移(Forward/Backward Transfer)、样本效率、参数利用率与稳定性。在多任务强化学习中,PNN等扩展方法常表现出显著的前向迁移(旧知识帮助新任务),并通过冻结旧列实现后向迁移保护(学新任务不伤旧任务),但代价是容量与推理成本的线性增长[^15][^18]。

表6 迁移与重用策略清单

| 策略 | 机制 | 优势 | 风险 | 适用场景 |
|---|---|---|---|---|
| 知识蒸馏(LwF)[^1] | 保持旧输出分布 | 不需旧数据、约束旧知识 | 分布漂移敏感、需稳健蒸馏 | 任务增量、在线更新 |
| 课程学习[^16] | 任务顺序与渐进训练 | 降低学习难度与遗忘 | 课程设计依赖先验 | 长序列任务、结构化场景 |
| 架构扩展(PNN等)[^15] | 新增容量与横向连接 | 强防遗忘、便于迁移 | 存储/推理成本增长 | 多任务强化学习、异构任务 |

---

## 第六部分:评估框架与最新研究进展

评估是持续学习的“地基”。主流设置包括任务增量(class-incremental)、领域增量(domain-incremental)、在线/流式学习(online/streaming),以及多模态融合。指标体系应兼顾平均精度、前向/后向迁移、样本效率、参数利用率、稳定性—可塑性平衡与遗忘程度;在强化学习中,还需关注回报分布与策略稳定性[^1]。

表7 持续学习评估设置与指标对照表

| 设置 | 说明 | 核心指标 | 代表基准 |
|---|---|---|---|
| 任务增量 | 任务边界清晰,标签集变化 | 平均准确率、遗忘度、后向迁移 | MNIST变体、CIFAR、ImageNet分层 |
| 领域增量 | 输入分布变化,类别不变 | 领域泛化、稳健性 | 视觉域适应基准 |
| 在线学习 | 数据流式到达,不可复现 | 累计回报、在线精度 | 流式学习数据集 |
| 强化学习 | 序列决策与奖励 | 人类归一化分数、稳定性 | Atari套件、RLBench类 |

在最新进展方面,三类方向值得强调。

第一,神经启发与类人遗忘。基于果蝇等物种的遗忘调控(如Rac活性调节)被用于构建“主动遗忘”的持续学习框架:通过在参数分布中适当衰减旧记忆,为新知识腾挪容量,并协调多学习器架构保持兼容性,显著提升任务增量下的性能与稳健性[^17]。这一方向从生物机制出发,提供了解决稳定性—可塑性冲突的新思路。

第二,类脑与脉冲神经网络的持续学习。脉冲神经网络(Spiking Neural Networks, SNN)与神经形态硬件为低功耗、事件驱动学习提供可能。近期研究提出持续学习范式(Replay4NCL)与脉冲架构(如SANTM),将记忆外部化与稀疏访问机制结合,针对长时程记忆与序列学习问题,探索结构—算法—硬件的协同设计[^18][^19][^20]。

第三,LLM持续学习的系统综述。随着大语言模型(Large Language Models, LLMs)规模与部署成本上升,持续学习成为保持模型时效与技能更新的关键路径。综述工作提出了多阶段分类方案,涵盖持续预训练、指令调优、对齐三个阶段,并讨论检索增强生成(RAG)、记忆外部化、参数高效适配(PEFT)等策略与评估基准[^21][^22][^23][^24]。同时,开源项目为社区提供了持续更新的资源索引与论文索引[^25]。

为便于整体把握,表8概述2024–2025年的关键研究与要点。

表8 2024–2025关键研究概览表

| 主题 | 代表工作 | 核心方法/机制 | 评估设置 | 主要结论 |
|---|---|---|---|---|
| 类人遗忘 | Nature Machine Intelligence (2023)[^17] | 多学习器主动遗忘、旧记忆衰减 | 任务增量(视觉/RL) | 稳定性—可塑性权衡改善,超越传统正则化 |
| 脉冲持续学习 | arXiv (2024/2025)[^18][^20] | SNN+记忆外部化、事件驱动回放 | 神经形态硬件模拟 | 低功耗持续学习可行,硬件友好 |
| 稀疏访问外部记忆 | SANTM (2025)[^19] | 稀疏访问、长时记忆 | 序列任务 | 记忆容量—访问效率平衡提升 |
| LLM持续学习 | Surveys (2024)[^21][^22][^23] | 持续预训练、指令调优、对齐;RAG/PEFT | NLP任务增量与在线更新 | 多阶段策略与评估框架完善 |
| 资源索引 | CL-LLM Survey Repo (2024)[^24] | 持续更新的论文/基准索引 | 全局 | 社区协作与知识共享平台 |
| 双侧记忆巩固 | CVPR (2023)[^26] | 双向巩固与架构协同 | 视觉分类增量 | 长期记忆整合能力增强 |
| LLM持续学习理论 | ACM (2024)[^22] | 理论分析与实践结合 | 多设置 | 规范化评估与风险识别 |

### 神经形态与外部记忆系统

类脑硬件与外部记忆架构为持续学习提供了“结构即策略”的新维度。神经形态平台强调事件驱动计算与低功耗,天然适配回放与门控;外部记忆系统(如稀疏访问神经图灵机)则为长时程记忆与序列学习提供高效的读写机制。与SNN结合时,需要关注精度—能耗权衡与硬件友好性设计,避免在仿真环境中忽略部署约束[^18][^19][^20]。

---

## 第七部分:综合洞见与战略建议

持续学习的核心,是在有限容量下,通过保护、重放与扩展三策协同,优化稳定性—可塑性权衡。参数约束(如EWC)以最小代价保护旧知识;生成式回放以低成本重演经验并实现价值导向选择;动态扩展以容量换取隔离与迁移的平衡。不同场景下需要“量体裁衣”的策略组合。

表9 策略组合指南表

| 场景 | 资源约束 | 风险点 | 推荐组合 | 备注 |
|---|---|---|---|---|
| 视觉任务增量(小数据) | 存储有限、无隐私 | 生成质量不稳 | EWC + 压缩回放 + 轻量蒸馏 | 关注压缩误差与Fisher估计偏差 |
| 视觉任务增量(大数据) | 存储可控 | 分布偏移 | EWC + 真实回放(小缓存) | 平衡缓存规模与隐私 |
| 强化学习(多任务) | 存储/推理预算有限 | 价值偏见 | PNN/参数隔离 + 价值引导回放 | 防止过度共享,控制容量增长 |
| NLP/LLM更新 | 计算/部署受限 | 遗忘/对齐风险 | 蒸馏(LwF)+ RAG/外部记忆 + PEFT | 降低重训练成本,强化评估基准[^21][^22] |
| 跨模态/跨域 | 对齐困难 | 表征失配 | 双侧/对称巩固 + 选择性回放 | 强调门控与分布校准[^26] |

在工程实践上,应重视以下要点:一是重视数据治理与评估闭环,避免离线提升在在线部署中打折扣;二是加强任务相似度感知的自适应λ调度与门控策略,减轻过约束与偏见;三是强化生成式回放的质量评估与分布校准,确保“重演”而非“误演”;四是针对LLM与多模态系统,构建统一的持续学习评估基准,纳入鲁棒性、效率与风险(如毒性、偏见、隐私)维度的量化指标[^1][^21][^22][^23]。

### 应用场景下的推荐方案

对于LLM/多模态系统,建议采用“轻约束—轻回放—外部记忆—参数高效适配(PEFT)”的组合:蒸馏与重要性约束保护旧知识,小规模回放或RAG提供上下文扩展,外部记忆维持长期一致性,PEFT降低适配与部署成本[^21][^22][^23]。在边缘计算与神经形态平台,推荐“事件驱动回放—稀疏外部记忆—结构化扩展”组合:以SNN与稀疏访问机制降低功耗与延迟,以架构扩展保障隔离与迁移,并通过任务级剪枝维持推理效率[^18][^19][^20]。

---

## 结语:未来方向与开放问题

展望未来,四条主线值得持续投入:一是统一跨模态与跨任务的持续学习评估标准,特别是纳入鲁棒性、效率、偏见与隐私的综合指标;二是深化神经机制—算法映射,尤其在选择性巩固与回忆门控的计算建模与实证验证方面,推动可解释的防遗忘机制;三是推进类脑—外部记忆—生成式回放的交叉融合,结合神经形态硬件探索低功耗、结构—算法—硬件协同的持续学习;四是重视工程实践中的风险治理,构建数据治理、评估闭环与部署监控的闭环体系[^1][^21]。

同时,我们也需正视当前的信息空白与研究需求:灾难性遗忘的分子—回路—系统耦合机制在更高分辨率的证据与统一理论模型上仍有待突破;EWC在高维、异构任务序列下的收敛与泛化保证尚不充分,亟需更稳健的Fisher或后验近似;生成式回放的分布对齐、偏见与隐私风险的系统评估框架仍需完善;参数隔离与架构扩展在大规模真实世界任务中的部署成本与长期维护仍需量化证据;LLM持续学习的统一基准与风险评估指标体系尚不成熟;跨模态/跨域持续学习的对齐与迁移度量尚未形成共识。这些空白既是挑战,也是推进持续学习走向工程成熟与科学可解释的机遇。

---

## 参考文献

[^1]: A Comprehensive Survey of Continual Learning: Theory, Method and Application. arXiv:2302.00487.  
[^2]: Replay, the default mode network and the cascaded memory systems model. Nature Reviews Neuroscience, 2022.  
[^3]: Synaptic Consolidation: An approach to long-term learning. PMC3368062.  
[^4]: Selective consolidation of learning and memory via recall-gated consolidation. eLife.  
[^5]: Time-dependent consolidation mechanisms of durable memory. Nature Communications Biology, 2025.  
[^6]: Diverse synaptic mechanisms underlying learning and memory consolidation. Current Opinion in Neurobiology, 2025.  
[^7]: Memory consolidation from a reinforcement learning perspective: simulation-selection model. Frontiers in Computational Neuroscience, 2024.  
[^8]: A neural network account of memory replay and knowledge consolidation. PMC9758580.  
[^9]: Learning offline: memory replay in biological and artificial systems. Trends in Neurosciences, 2021.  
[^10]: Overcoming catastrophic forgetting in neural networks (Elastic Weight Consolidation, EWC). PNAS, 2017.  
[^11]: Note on the quadratic penalties in elastic weight consolidation. PNAS, 2018.  
[^12]: Learning offline: memory replay in biological and artificial systems. Trends in Neurosciences, 2021.  
[^13]: Learning offline: memory replay in biological and artificial systems. Trends in Neurosciences, 2021.  
[^14]: Memory Replay with Data Compression for Continual Learning. arXiv:2202.06592.  
[^15]: Progressive Neural Networks. arXiv:1606.04671.  
[^16]: Progressive learning: A deep learning framework for continual learning. Neural Networks, 2020.  
[^17]: Incorporating neuro-inspired adaptability for continual learning. Nature Machine Intelligence, 2023.  
[^18]: Continual Learning with Neuromorphic Computing. arXiv:2410.09218.  
[^19]: SANTM: A Sparse Access Neural Turing Machine with local learning. Applied Soft Computing, 2025.  
[^20]: Replay4NCL: An Efficient Memory Replay-based Continual Learning Framework. arXiv:2503.17061.  
[^21]: Continual Learning for Large Language Models: A Survey. arXiv:2402.01364.  
[^22]: Continual Learning for Large Language Models. ACM Digital Library, 2024.  
[^23]: ContinualAI/llm-continual-learning-survey. GitHub, 2024.  
[^24]: A Comprehensive Survey of Continual Learning. IEEE Xplore, 2024.  
[^25]: Bilateral Memory Consolidation for Continual Learning. CVPR, 2023.  
[^26]: Learning offline: memory replay in biological and artificial systems. Trends in Neurosciences, 2021.