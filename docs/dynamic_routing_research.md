# 动态路由与计算路由前沿理论:强化学习模块选择、动态网络、稀疏MoE、自适应计算、能效优化与实时技术的系统综述

## 执行摘要与研究结论总览

随着大模型、多模态学习与边缘-云协同系统的快速演进,计算“路由”(即针对输入或任务的计算路径与专家模块选择)与网络“路由”(即端到端数据包的路径规划)正在以全新的方式耦合与收敛。前者以内生稀疏激活与门控为核心,后者以外生约束(带宽、时延、能耗、可靠性)为关键,两者共同指向一个一致的目标:在有限资源下,以数据相关的方式动态分配计算与通信,达成性能与成本的最佳权衡。

三条清晰洞见贯穿全文。第一,数据相关的动态计算分配是可扩展性的主线。以稀疏专家(Mixture of Experts, MoE)为代表的条件计算,使得每个输入仅激活少量专家,从而在参数规模扩展的同时维持每样本计算稳定;Vision MoE(V-MoE)等工作验证了“稀疏激活+路由扩展”的可扩展性与推理减负潜力[^1]。第二,路由范式需要融合优化与学习。无线传感器网络(Wireless Sensor Networks, WSN)中的最新研究表明,将强化学习(Reinforcement Learning, RL)与遗传算法(Genetic Algorithm, GA)、粒子群优化(Particle Swarm Optimization, PSO)等群体智能算法组合在统一路由框架中,能够显著提升端到端时延、数据包交付率与能耗表现,并在节点资源受限条件下仍保持实用性与可扩展性[^2]。第三,系统实现必须统一时延与能效度量,并在边缘-云-无线复杂环境中联合优化。WDMoE(无线分布式MoE)给出了门控、带宽与权重-时延比(Weight-to-Latency Ratio, WLR)协同优化的设计空间,证明在保持模型能力的前提下显著降低推理时延[^3]。

关键证据速览如下。V-MoE在视觉Transformer中引入稀疏MoE路由,并提出面向批次内优先级的路由扩展以实现“按图像自适应计算”;在ImageNet上达到90.35%准确率的同时,推理计算开销约为密集模型的一半[^1]。DEL(自适应动态集成学习)通过类特定阈值与动态剪枝,将推理延迟降低约47%,并带来约37%的剪枝加速,同时提升准确率与降低损失[^5]。在WSN中,模块化AI路由框架在数据包交付率、端到端时延与能耗方面均显著优于传统协议,并可在低内存节点上稳定运行[^2]。在分布式无线推理场景,WDMoE通过联合优化专家选择与带宽分配,以WLR为路由指标实现约45.75%的延迟降低且不损失模型性能[^3]。

应用落地前景清晰:在云端大模型推理中,稀疏MoE可提升吞吐并控制成本;在边缘-云协同的无线分布式推理中,WDMoE式协同优化将成为关键;在WSN与物联网(IoT)中,AI路由与强化学习优化能延长网络寿命并增强可靠性。本文在后续章节给出面向六大主题的系统性综述与比较,并在末尾提供实施路线图与评估框架,覆盖MLOps、系统设计与网络工程的可操作建议。

## 方法论与来源审查

本综述围绕六大主题展开:基于强化学习的模块选择策略、动态神经网络架构、稀疏专家网络(MoE)、自适应计算分配、能效优化的路径规划与实时动态路由技术。我们对来源进行分级评估并交叉验证:优先采用同行评审期刊与会议论文(如Scientific Reports、Knowledge-Based Systems、AAAI、IJCAI),其次为arXiv预印本与部分中文技术综述博客(仅用于术语补充与工程经验)。时间基准以2025-11-16为当前参考,重点关注2023–2025的最新进展。

证据甄别策略包括:一方面通过跨来源的指标与公式对比以识别一致性,例如延迟、能效、扩展性与路由度量的定义;另一方面,针对工程实践,参考可复现的开源实现与工程案例以校准理论与落地之间的距离。典型地,我们在动态多模态融合与早期退出(Early Exiting)主题中,既引用ACM与AAAI论文,也辅以综述性资源与代码库以增强可操作性[^16][^13][^14]。

需要强调的信息空白包括:在“基于强化学习的模块选择策略”方面,直接面向神经网络路由/模块选择的RL算法选择与实证细节较为分散,尤其是在大规模分布式MoE门控网络上的奖励设计、稳定性与收敛性仍需更多权威基准支撑;实时动态路由的大规模在线学习系统级复现实验仍有限;能耗建模在跨层硬件参数、无线链路波动与多模态负载融合方面的统一度量尚待标准化。

为便于读者把握来源覆盖与质量,我们汇总了主题与来源类型的矩阵,并对每一类给出示例与简要评估。

表1:来源类型与质量评估矩阵(主题覆盖与证据强度)

| 主题 | 同行评审 | arXiv | 期刊/会议综述/代码 | 证据强度与覆盖 |
|---|---|---|---|---|
| RL模块选择策略 | 部分(工程优化期刊)[^20] | RL×MoE探索[^10] | 算法选择研究[^19] | 中等:理论与工程均有,但需更多分布式门控RL实证 |
| 动态神经网络架构 | NAS系统综述[^11];AlphaX[^12];ACM早退综述[^13] | 动态多模态融合[^6] | 早退代码与资源[^14] | 高:理论与工程均覆盖,多来源一致性良好 |
| 稀疏MoE | MoE综合综述[^9];V-MoE[^1] | WDMoE[^3] | Token-Fusion路由[^8] | 高:跨模态与系统设计均有实证与方法 |
| 自适应计算分配 | AAAI 2025 KS-NAS[^4] | WDMoE[^3] | DEL框架[^5] | 高:方法多样,指标明确,工程可操作 |
| 能效优化路径规划 | Scientific Reports[^2];路由优化[^7];Nature路由优化[^17];AI路由算法[^18] | — | — | 高:跨网络场景实证充分,指标全面 |
| 实时动态路由 | WDMoE[^3] | — | — | 中:分布式无线推理的时延证据明确,但需要更大规模部署数据 |

上述矩阵显示,整体证据覆盖度高,尤其在稀疏MoE、自适应计算分配与能效路由三个方面;而RL模块选择与实时系统级复现仍需进一步补充与标准化。

## 统一概念框架:从网络路由到计算路由

在传统网络路由中,路径选择受到带宽、时延、可靠性与能耗等外生约束;在计算路由中,路由对象由“数据包”变为“输入/token/样本/模态”,路由目标转为在有限计算资源下动态选择模块或专家,以实现性能-成本的最优权衡。两者的本质都是“在约束下进行动态分配”,但在度量口径与系统实现上存在显著差异。

- 网络路由关注:端到端路径的数据包交付率、时延、吞吐与能耗;典型度量包含路径代价、队列长度、误码率与能量预算。
- 计算路由关注:每样本/每token的激活模式(稀疏或动态)、置信与不确定性、门控负载均衡与路由熵;典型度量包含专家利用率、路由稳定性、提前退出置信度与动态剪枝比例。

在融合范式中,上述两类路由通过“条件计算+分布式推理+约束优化”形成闭环。WDMoE的工作正是将计算路由中的门控与网络路由中的带宽分配联合优化,以权重-时延比(WLR)作为统一路由度量:门控输出权重既反映“计算价值”,又通过无线链路的时延特性映射为“等待成本”,进而在专家选择与带宽配置上形成端到端的一致优化[^3]。在多模态场景中,DynMM(动态多模态融合)则通过数据相关的前向路径与资源感知损失函数,将“模态级/融合级”的动态决策显式化,从而在不同数据分布下自动调节计算路径并压缩成本[^6]。在早期退出中,置信、熵与耐心阈值(patience)构成“计算预算与精度风险”的动态边界,形成另一个计算路由的稳定子范式[^13][^15]。

这一统一框架的核心是“数据相关的动态分配”。MoE以稀疏激活最大化参数规模而限制每样本计算;DynMM以路径生成实现模态层面的自适应计算;Early Exiting以退出策略实现层级的自适应计算;DEL以类特定阈值与动态剪枝实现模型层面的自适应计算。它们共同回答一个根本问题:如何让每个输入只做“必要的工作”,在限定资源下达到“足够好”的结果。

## 基于强化学习的模块选择策略

强化学习为“模块/专家/算子选择”提供了端到端的策略学习机制。其关键在于将选择问题形式化为马尔可夫决策过程(MDP),在给定状态下选择动作(模块或算子),获得奖励(性能提升、时延/能耗惩罚与负载均衡),并通过值函数或策略梯度进行优化。

在超启发式调度中,一种基于Actor-Critic的策略梯度超启发式算法(PGHHA)以高层策略动态选择低层启发式算子(全局搜索×局部挖掘×邻域深度),并用优先经验回放加速收敛。该方法在多目标与不确定性环境中展现出更好的收敛性与解多样性,为“选择哪些模块/算子、何时切换”提供了策略学习范式[^20]。进一步地,在模块化路由的Actor-Critic框架中,采用门控MoE可动态激活专家集,使得策略学习与专家激活协同优化;最新综述与工程实践也强调此类结构在提升非平稳学习能力与减少“休眠神经元”方面的潜力[^10]。

奖励设计是多目标优化的核心。典型信号包括:准确率或返回质量的提升、时延与能耗的惩罚项、负载均衡项(用于避免专家或节点的过载与欠载)、以及路由熵正则(用于控制路由的稳定性与不确定性)。算法选择方面,值函数方法(如Q学习)易于分析但动作空间较大时可能受限;策略梯度(含Actor-Critic)更适于连续或大规模动作选择;离线强化学习可提升部署稳定性并减少在线探索风险[^19]。

表2:RL模块选择方法比较(范式、奖励信号、训练稳定性、计算开销)

| 方法 | 形式 | 核心奖励信号 | 稳定性要点 | 计算开销 | 适用场景 |
|---|---|---|---|---|---|
| 值函数(Q学习) | 离散动作 | 质量提升、时延惩罚、负载均衡 | 动作空间膨胀时收敛慢 | 低-中 | 小规模模块选择、网络局部决策[^19] |
| 策略梯度(Actor-Critic) | 连续/大动作空间 | 多目标奖励(质量/时延/能耗/熵) | 需熵正则与优势函数稳定训练 | 中-高 | 算子/专家动态选择、在线调度[^20] |
| 离线RL | 批处理 | 风险控制与稳健性 | 依赖高质量离线数据 | 中 | 高风险场景的稳定部署[^19] |

上述比较表明:在“大动作空间+多目标奖励+在线适配”的工程情境下,Actor-Critic策略梯度更具灵活性;在“资源受限+风险厌恶”的情境下,离线RL提供更稳健的路径;值函数方法则为结构清晰、规模可控的决策问题提供了简洁解。

## 动态神经网络架构:早期退出、动态融合与NAS

早期退出(Early Exiting)以“在中间层预测”作为加速机制,通过置信、熵或耐心阈值控制退出时机。其关键在于为不同样本匹配不同计算深度,在不显著损失精度的前提下降低平均推理时间。系统综述表明,Early Exiting高度依赖阈值设定与退出层设计,且需要在训练阶段显式加入中间监督信号以保证早期预测的有效性[^13]。工程经验如PABEE提出以“耐心”参数控制退出行为,在一定加速比下可以实现优于完整模型的表现;同时,DeeBERT与FastBERT等工作探索了自蒸馏与自适应推理时间,使早期层与后层之间的表示学习更为一致[^15]。在持续学习场景中,早期退出与防遗忘之间存在“双重收益”的可能性:适度的早期层稳定化可以减少遗忘,同时降低推理成本[^16]。

动态多模态融合(DynMM)以“数据相关的前向路径生成”为核心,采用门控函数在模态级与融合级进行决策,并引入资源感知损失函数显式鼓励计算效率提升。该方法在情感分析与语义分割任务中分别实现了约46.5%与>21%的计算节省,同时保持精度损失可忽略不计[^6]。这揭示了跨模态的“路径级自适应”可显著压缩成本,特别适合多源数据质量差异较大的实际场景。

神经架构搜索(Neural Architecture Search, NAS)与动态架构的结合是另一条关键路径。系统综述表明,NAS在样本效率与评估成本上面临挑战;AlphaX以深度神经网络与蒙特卡罗树搜索(MCTS)为核心,提升搜索效率并降低评估成本,其Meta-DNN用于预测网络精度、引导搜索向更有希望的域推进[^11][^12]。在多模态分类任务中,KS-NAS通过引入动态更新的知识库、并在深度进化搜索中从知识库初始化参数,显著加速种群训练,同时保证分类性能与训练效率达到最先进结果[^4]。

表3:动态架构方法对比(早期退出/DynMM/NAS)

| 方法 | 关键思想 | 训练机制 | 推理加速与精度 | 资源消耗 | 备注 |
|---|---|---|---|---|---|
| Early Exiting | 中间层预测+阈值退出 | 中间监督、自蒸馏 | 加速显著、精度可控 | 中 | 阈值与退出层设计敏感[^13][^15] |
| DynMM | 数据相关路径+资源感知损失 | 门控函数+跨模态融合 | 计算节省可观、精度损失小 | 中 | 适合多源数据质量差异[^6] |
| NAS/AlphaX | DNN+MCTS+元预测 | 样本效率与评估优化 | 搜索高效、架构更优 | 中-高 | 评估成本高但可控[^12] |
| KS-NAS | 知识库初始化+深度进化搜索 | 动态知识库+参数聚合 | 训练显著加速、性能SOTA | 中 | 多模态分类任务验证[^4] |

这一对比揭示:Early Exiting适合层内计算压缩,DynMM适合跨模态路径压缩,NAS适合结构级优化;三者协同可在不同层次上实现“自适应计算”。

## 稀疏专家网络(MoE):路由与扩展性

稀疏MoE通过门控网络稀疏选择少量专家,使每个输入仅激活少数参数子集,既扩展参数规模又保持每样本计算稳定。路由机制通常包含负载均衡与路由熵正则,以避免过度集中导致专家“热专家”问题。在视觉领域,V-MoE作为Vision Transformer的稀疏版本,展示了与最大密集网络竞争的性能,并提出批次内优先级路由扩展以实现“按图像自适应计算”;在ImageNet上达到90.35%准确率,同时推理计算开销约为密集网络的一半[^1]。

在多任务与多模态路由中,Token-Fusion提出面向跨任务数据匹配的稀疏专家路由方法,强调在多源数据匹配场景下的专家选择与融合策略优化[^8]。在无线分布式场景中,WDMoE将基站的门控与移动设备的专家通过无线链路连接,联合优化专家选择与带宽分配,以WLR作为路由指标。在下行链路速率由带宽与信道增益决定的前提下(形式化表达为 R_k^d = B_k · log2(1 + P_k^d · g_{BS,k} / (N_0 · B_k))),通过门控权重与等待时延的双层优化,延迟降低约45.75%且不损失模型能力[^3]。最新综合综述系统总结了MoE的门控函数、专家网络、路由机制、训练策略与系统设计,指出在训练与推理系统中需权衡负载均衡、路由稳定与硬件利用率[^9]。

表4:MoE路由算法对比(V-MoE/Token-Fusion/WDMoE)

| 方法 | 路由信号 | 负载均衡 | 时延优化 | 精度影响 | 系统约束 |
|---|---|---|---|---|---|
| V-MoE | 门控权重+批次优先级 | 批次级均衡 | 测试时性能-计算平滑权衡 | 与密集网络相当或更优 | 图像/视觉任务;数据中心推理[^1] |
| Token-Fusion | token级专家路由 | 跨任务均衡 | 路由与融合协同优化 | 多任务匹配提升 | 多源数据匹配任务[^8] |
| WDMoE | 权重-时延比(WLR) | 设备级与专家级均衡 | 联合带宽分配,延迟降低约45.75% | 不损失模型能力 | 无线分布式、移动设备专家[^3] |

MoE的价值在于“可扩展的参数+可控的每样本计算”,而路由策略是实现这一价值的关键“调度器”。在不同场景下,路由信号与系统约束的联动决定了性能-时延-能耗的平衡位置。

## 自适应计算分配:从方法到指标

自适应计算的目标是“为每个输入分配恰当的计算量”,以最少的平均计算获得可接受的精度。代表性方法包括:Early Exiting(按层退出)、DynMM(按路径生成)、MoE(按专家稀疏激活)、DEL(按类阈值与动态剪枝)。这些方法的度量信号涵盖置信度、熵、耐心阈值、类特定阈值与路由负载指标。

DEL在图像分类任务中提出“类特定模型选择策略”,即根据每个分类器在特定类别上的历史表现设定阈值并激活模型;同时引入动态剪枝机制以减少推理延迟。实证显示,DEL在多个数据集上提升准确率与降低损失;在Fashion-MNIST上,最佳阈值约为0.90,整体延迟降低约47%,动态剪枝机制带来约37%的额外延迟降低[^5]。这为“类特定计算分配”提供了可复用的范式。

DynMM与KS-NAS则在“多模态与结构搜索”层面提供了自适应计算分配。DynMM通过资源感知损失与路径生成显著压缩计算成本(在CMU-MOSEI任务中降低约46.5%),而KS-NAS通过动态知识库与参数聚合减少从头训练的成本,加速种群进化[^6][^4]。

表5:自适应计算方法与指标映射

| 方法 | 关键信号 | 平均计算节省 | 精度变化 | 工程部署要点 |
|---|---|---|---|---|
| Early Exiting | 置信/熵/耐心阈值 | 中-高(依阈值) | 可控下降 | 中间监督与阈值选择敏感[^13][^15] |
| DynMM | 门控输出+资源感知损失 | ~46.5%(CMU-MOSEI) | 可忽略不计 | 门控与路径生成需协同训练[^6] |
| MoE | 门控权重+路由熵+负载均衡 | 中-高(依稀疏度) | 维持或提升 | 路由器与专家利用率权衡[^1][^9] |
| DEL | 类特定阈值+动态剪枝 | 延迟降低约47% | 准确率提升、损失下降 | 阈值调优与剪枝策略稳定性[^5] |
| KS-NAS | 知识库初始化+进化搜索 | 训练加速显著 | 性能SOTA | 知识库更新与评估成本控制[^4] |

上述映射表明:自适应计算的“信号-节省-精度-部署”四元关系是可操作与可评估的,适合在MLOps与系统设计中落地。

## 能效优化的路径规划:WSN/IoT与AI路由

在WSN/IoT中,能量与时延、交付率与鲁棒性构成路由设计的多元目标。最新的模块化AI路由框架提出了“本地RL决策+全局群体智能优化”的组合策略:在节点级采用Q学习进行分散式决策;在全局层面用GA/PSO优化路径与资源分配。仿真环境采用一阶无线电模型与标准节点配置(TelosB、Mica2),实验结果显示:数据包交付比例显著提升、端到端时延显著降低、能耗下降并延长网络寿命,且在低内存与低功耗节点上保持计算效率与鲁棒性[^2]。在传统最短路径类方法上,能量感知的自适应Dijkstra算法也通过动态权重更新提升鲁棒性与能效表现[^7]。更广泛地,AI路由算法被证明可提升能源效率与整体网络可靠性与可持续性[^18];在复杂网络场景中,强化学习路由优化模型在不同场景下保持网络性能并实现能效优化[^17]。

表6:WSN/IoT能效路由性能对比

| 方法 | 数据包交付率 | 端到端时延 | 能耗 | 网络寿命 | 资源需求 | 备注 |
|---|---|---|---|---|---|---|
| 模块化AI路由(RL+GA/PSO) | 显著优于DVR/LSR/ACO | 显著降低 | 显著降低 | 延长 | RAM~8–12KB;CPU周期<2000/决策 | 在TelosB/Mica2上运行稳定[^2] |
| 能量感知自适应Dijkstra | 提升 | 降低 | 降低 | 延长 | 传统节点可用 | 基于加权最短路径[^7] |
| 强化学习路由优化 | 提升 | 降低 | 降低 | 延长 | 依实现而定 | 跨场景有效性验证[^17] |
| AI路由算法 | 提升 | 降低 | 降低 | 延长 | 依算法而定 | 可靠性与可持续性增强[^18] |

这一对比强调:在资源受限与动态环境下,AI路由通过学习与优化组合能获得系统级综合收益,相比传统协议在能效与可靠性上更有优势。

## 实时动态路由的最新技术:端到端优化

在分布式无线推理场景中,WDMoE联合优化门控与带宽,提出权重-时延比(WLR)这一综合路由度量:门控权重代表“计算的相对重要性”,而设备处理与链路等待时延代表“时延成本”。通过双层优化(上层最小化注意力等待时延,下层确保模型能力),在不损失模型能力的前提下,延迟降低约45.75%[^3]。这一机制展示了“计算路由与网络路由”的真正融合:门控的输出与无线链路的物理约束形成统一的决策空间。

同时,模块化AI路由在WSN的端到端路径规划中也显著降低时延与能耗、提升交付率,表明在网络层与应用层联合优化可在实时场景下保持高可靠性与可扩展性[^2]。

表7:实时路由技术对比(WDMoE vs. WSN路由)

| 场景 | 路由度量 | 资源约束 | 延迟表现 | 精度影响 | 可扩展性 |
|---|---|---|---|---|---|
| WDMoE分布式推理 | 权重-时延比(WLR) | 无线带宽、信道增益、设备处理能力 | 降低约45.75% | 不损失模型能力 | 面向移动设备与基站协同[^3] |
| WSN路由 | 能耗、路径代价、队列长度 | 节点能量、内存与计算限制 | 显著降低 | 保持或提升交付率 | 面向大规模节点部署[^2] |

这一对比揭示:不同系统中的“实时性”要求可通过统一的路由度量(如WLR)与跨层优化实现,工程上建议从边端能力与链路约束出发进行适配。

## 系统实现与工程实践建议

要在生产环境中部署动态路由与计算路由,需要同时关注系统栈的各层协同与MLOps的闭环管理。

- 模型层:MoeLayer的路由与负载均衡是核心,路由器应实现负载均衡正则与熵正则,并通过利用率监控防止“热专家”;门控稳定性与路由熵需要在训练与推理阶段统一考虑[^9]。
- 编译与图优化:开启算子融合与内核优化,结合流水线与并行策略;在Early Exiting场景下,需在图级别显式管理中间出口的调度与缓存。
- 分布式系统:在边缘-云协同的无线推理中,参考WDMoE的联合优化,将门控、带宽与权重-时延比作为系统级指标进行调度;对移动设备的专家计算与基站的注意力模块进行负载切分与时延对齐[^3]。
- 监控与回退:部署类特定阈值与剪枝比例的在线监控,设置路由退化与负载失衡告警;在性能退化或负载异常时自动回退到密集路径或保守阈值;引入版本化路由策略以支持A/B测试与快速回滚。
- MLOps闭环:数据分布漂移与场景迁移需触发阈值重标定与路由器再训练;知识库更新的自动化(如KS-NAS范式)与评估流程需纳入持续集成与发布[^4]。

在可复现与开源资源方面,MoE的开源实现与早退论文列表提供了具体工程参考;建议以这些资源为基线,结合自身场景进行增量开发与评测[^22][^14]。

## 风险、挑战与开放问题

- 稳定性与收敛性:RL路由在非平稳与多目标奖励环境下的稳定性仍具挑战;负载均衡与路由熵正则之间的权衡需要更系统的理论支撑与实证基准[^9][^10]。
- 安全与鲁棒性:在无线链路波动与能耗受限环境中,AI路由面临安全与鲁棒性双重风险;需要更完善的入侵检测与容错机制以保证长期稳定性[^2]。
- 度量标准化:跨层能耗度量尚未统一,尤其在硬件参数、无线信道与多模态负载融合方面;建议从“能耗/延迟/吞吐/可靠性”的四元指标体系出发,建立任务级与系统级的对齐标准[^2]。
- 部署成本与运维:动态路由系统需要在监控、回退与版本化治理上投入工程成本;在边缘-云协同与无线分布式推理中,链路与设备的不确定性要求更高的自动化与自适应能力。

信息空白方面,需要强调:在大规模分布式MoE门控网络中,RL路由的奖励设计、稳定性与收敛性的权威基准尚不足;早期退出与DynMM在2025年更多大规模NLP任务的系统级对比与可复现数据仍需补充;MoE扩展定律与内存约束下的配置指导需要更系统的开源对照;实时动态路由在边缘-云-无线跨域场景的端到端指标与统一评测框架仍待完善;工业案例与开源实现清单建议在后续调研中扩充。

## 战略建议与实施路线图

为了在云端大模型与边缘-云协同推理中实现可扩展与绿色的动态路由,建议采用“试点—评估—扩展—运营化”的分阶段路线。

- 短期试点:在选定任务中部署MoE与Early Exiting,并以DEL进行动态剪枝与类特定阈值管理;在网络侧选择代表性WSN场景进行AI路由试点,建立基线指标(时延、能耗、交付率、稳定性)。
- 中期集成:将动态路由纳入MLOps闭环,引入KS-NAS的知识库初始化与深度进化搜索以加速架构优化;在分布式无线推理场景落地WDMoE的联合优化(门控+带宽+WLR),形成端到端的统一度量与调度器。
- 长期演进:建立跨层能耗与时延的统一度量标准与评估框架;持续完善开源生态与可复现资源,推动基准测试与行业标准的建设。

表8:实施路线图与里程碑(阶段、目标、关键任务、评估指标、风险缓解)

| 阶段 | 目标 | 关键任务 | 评估指标 | 风险缓解 |
|---|---|---|---|---|
| 短期试点 | 建立基线与可行性 | MoE/Early Exiting/DEL部署;WSN路由试点 | 准确率/延迟/能耗/交付率 | 严格A/B测试与回滚机制[^1][^5][^2] |
| 中期集成 | 纳入MLOps与端到端优化 | KS-NAS知识库自动化;WDMoE联合优化 | 训练成本/推理时延/负载均衡 | 版本化路由策略与告警体系[^4][^3] |
| 长期演进 | 标准化与生态完善 | 统一度量与评估框架;开源资源扩展 | 能效/稳定性/可扩展性 | 持续基准测试与行业共建[^9][^22][^14] |

该路线图强调:以可操作的评估指标为主线,以知识库与联合优化为技术抓手,以版本化与告警为工程保障,实现从试点到规模化的稳健推进。

## 附录:术语、符号与评估指标

术语表(部分):
- 路由熵:用于衡量路由分布的不确定性,低熵表示更稳定的路由选择。
- 权重-时延比(WLR):综合门控输出权重与设备/链路等待时延的路由度量。
- 早期退出(Early Exiting):在中间层设置预测器,基于置信、熵或耐心阈值决定是否提前输出结果。
- 动态多模态融合(DynMM):通过门控函数与资源感知损失,生成数据相关的前向路径以实现自适应计算。
- 混合专家(MoE):通过门控稀疏选择专家子集,以扩展参数规模并控制每样本计算。
- 分层早退阈值:在不同退出层设置不同的置信或耐心阈值以平衡精度与时延。

符号与度量映射表:

| 符号/度量 | 含义/定义 | 适用场景 | 参考 |
|---|---|---|---|
| R_k^d | 下行传输速率 R_k^d = B_k · log2(1 + P_k^d · g_{BS,k} / (N_0 · B_k)) | 无线分布式推理 | WDMoE[^3] |
| WLR | 权重-时延比:门控权重与等待时延的综合路由指标 | 分布式推理与移动设备 | WDMoE[^3] |
| 熵(路由) | 路由分布不确定性,用于稳定性与负载均衡正则 | MoE路由器训练 | MoE综述[^9] |
| 置信/熵阈值 | Early Exiting的退出判定标准 | 层内自适应计算 | ACM综述[^13] |
| 耐心阈值(patience) | 持续若干步满足条件则退出 | 早退策略 | 综述博客[^15] |
| 类特定阈值 | 针对类别的激活阈值,用于DEL | 动态集成选择 | KBS[^5] |
| HVR/IGD | 多目标优化的超体积比率与反向世代距离 | 调度优化评估 | JEIT[^20] |

## 参考文献

[^1]: Riquelme, C., et al. Scaling Vision with Sparse Mixture of Experts. arXiv:2106.05974. https://arxiv.org/abs/2106.05974
[^2]: Scientific Reports. AI-based routing algorithms improve energy efficiency, reliability, adaptability, and sustainability of WSNs. 2025;15:22292. https://www.nature.com/articles/s41598-025-08677-w
[^3]: WDMoE: Wireless Distributed Mixture of Experts for Large Language Models. arXiv:2411.06681. https://arxiv.org/html/2411.06681v1
[^4]: IJCAI 2025. KS-NAS: Divide-and-conquer neural architecture search for multi-modal classification. https://dl.acm.org/doi/10.24963/ijcai.2025/557
[^5]: Knowledge-Based Systems (2025). Adaptive Dynamic Ensemble Learning with Class-Specific Thresholds. https://www.sciencedirect.com/science/article/pii/S0950705125018805
[^6]: Dynamic Multimodal Fusion (DynMM). arXiv:2204.00102. https://arxiv.org/abs/2204.00102
[^7]: IEEE Xplore. Energy-Aware Adaptive Dijkstra: A Robust Routing Algorithm for WSNs. https://ieeexplore.ieee.org/abstract/document/10527647
[^8]: Token-Fusion: A Sparse Expert Routing Method for Multi-task Data Matching. ACM. https://dl.acm.org/doi/epdf/10.1145/3746252.3760924
[^9]: A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications. arXiv:2503.07137. https://arxiv.org/abs/2503.07137
[^10]: Mixture of Experts in a Mixture of RL settings. arXiv:2406.18420. https://arxiv.org/html/2406.18420v1
[^11]: Artificial Intelligence Review. Systematic review on neural architecture search (2017–2023). https://link.springer.com/article/10.1007/s10462-024-11058-w
[^12]: AlphaX: Neural Architecture Search using Deep Neural Networks and Monte Carlo Tree Search. arXiv:1805.07440. https://arxiv.org/abs/1805.07440
[^13]: Early-Exit Deep Neural Network: A Comprehensive Survey. ACM. https://dl.acm.org/doi/abs/10.1145/3698767
[^14]: Early-Exit Papers (GitHub curated list). https://github.com/falcon-xu/early-exit-papers
[^15]: 模型早退技术综述:经典动态早退机制介绍(择数AI). https://zhuanlan.zhihu.com/p/439166201
[^16]: Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Architectures. arXiv:2403.07404. https://arxiv.org/html/2403.07404v1
[^17]: Nature Scientific Reports. Reinforcement learning based route optimization model. https://www.nature.com/articles/s41598-025-86608-5
[^18]: An AI-based approach for dynamic routing in IoT networks. Springer. https://link.springer.com/article/10.1007/s12083-025-01942-9
[^19]: Microsoft Research. Reinforcement Learning Algorithm Selection. https://www.microsoft.com/en-us/research/publication/reinforcement-learning-algorithm-selection/
[^20]: 电子与信息学报(JEIT). 策略梯度超启发式调度:Actor-Critic动态选择低层启发式策略. https://jeit.ac.cn/cn/article/doi/10.11999/JEIT250769
[^21]: CSDN博客. 神经启发深度学习的节能与安全路由框架(ScienceDirect摘要页). https://www.sciencedirect.com/science/article/pii/S0952197625028143
[^22]: GitHub. MoE-Mixture-of-Experts in PyTorch(开源实现). https://github.com/junfanz1/MoE-Mixture-of-Experts-in-PyTorch