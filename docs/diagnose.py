#!/usr/bin/env python3
"""
Windows 11 Brain AI ç¯å¢ƒè¯Šæ–­å·¥å…·
å®Œæ•´ç‰ˆè¯Šæ–­è„šæœ¬ï¼ŒåŒ…å«æ‰€æœ‰æ£€æŸ¥é¡¹ç›®
"""

import sys
import subprocess
import platform
import os
import time
import json
import psutil
from datetime import datetime

# å°è¯•å¯¼å…¥å¿…è¦çš„åº“
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

class BrainAIDiagnosis:
    def __init__(self):
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'system_info': {},
            'python_environment': {},
            'dependencies': {},
            'gpu_status': {},
            'performance': {},
            'issues': [],
            'recommendations': []
        }
    
    def check_system_info(self):
        """æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯"""
        print("ğŸ” æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯...")
        
        try:
            system_info = {
                'os': platform.system(),
                'os_version': platform.release(),
                'architecture': platform.machine(),
                'processor': platform.processor(),
                'python_version': sys.version,
                'python_executable': sys.executable,
                'ram_total': f"{psutil.virtual_memory().total / (1024**3):.1f} GB",
                'ram_available': f"{psutil.virtual_memory().available / (1024**3):.1f} GB",
                'cpu_count': psutil.cpu_count(),
                'cpu_frequency': f"{psutil.cpu_freq().current:.0f} MHz" if psutil.cpu_freq() else "Unknown"
            }
            
            self.results['system_info'] = system_info
            
            print(f"âœ… æ“ä½œç³»ç»Ÿ: {system_info['os']} {system_info['os_version']}")
            print(f"âœ… æ¶æ„: {system_info['architecture']}")
            print(f"âœ… å†…å­˜: {system_info['ram_total']}")
            print(f"âœ… CPU: {system_info['cpu_count']} cores @ {system_info['cpu_frequency']}")
            print(f"âœ… Python: {platform.python_version()}")
            
        except Exception as e:
            self.results['issues'].append(f"ç³»ç»Ÿä¿¡æ¯æ£€æŸ¥å¤±è´¥: {str(e)}")
            print(f"âŒ ç³»ç»Ÿä¿¡æ¯æ£€æŸ¥å¤±è´¥: {str(e)}")
    
    def check_python_environment(self):
        """æ£€æŸ¥Pythonç¯å¢ƒ"""
        print("\nğŸ æ£€æŸ¥Pythonç¯å¢ƒ...")
        
        try:
            env_info = {
                'python_path': sys.executable,
                'python_version': sys.version,
                'pip_version': subprocess.run(['pip', '--version'], 
                                            capture_output=True, text=True).stdout.strip(),
                'virtual_env': sys.prefix != sys.base_prefix,
                'path': sys.path
            }
            
            self.results['python_environment'] = env_info
            
            print(f"âœ… Pythonè·¯å¾„: {sys.executable}")
            print(f"âœ… è™šæ‹Ÿç¯å¢ƒ: {'æ˜¯' if env_info['virtual_env'] else 'å¦'}")
            print(f"âœ… pipç‰ˆæœ¬: {env_info['pip_version'].split()[1]}")
            
        except Exception as e:
            self.results['issues'].append(f"Pythonç¯å¢ƒæ£€æŸ¥å¤±è´¥: {str(e)}")
            print(f"âŒ Pythonç¯å¢ƒæ£€æŸ¥å¤±è´¥: {str(e)}")
    
    def check_dependencies(self):
        """æ£€æŸ¥ä¾èµ–åŒ…"""
        print("\nğŸ“¦ æ£€æŸ¥ä¾èµ–åŒ…...")
        
        required_packages = [
            'torch', 'torchvision', 'numpy', 'scipy', 'pandas',
            'sklearn', 'matplotlib', 'jupyter', 'click', 'pyyaml',
            'tqdm', 'rich', 'psutil'
        ]
        
        dependencies = {}
        
        for package in required_packages:
            try:
                if package == 'sklearn':
                    import sklearn
                    version = sklearn.__version__
                else:
                    module = __import__(package)
                    version = getattr(module, '__version__', 'Unknown')
                
                dependencies[package] = {
                    'installed': True,
                    'version': version,
                    'status': 'âœ…'
                }
                print(f"âœ… {package}: {version}")
                
            except ImportError:
                dependencies[package] = {
                    'installed': False,
                    'version': None,
                    'status': 'âŒ'
                }
                print(f"âŒ {package}: æœªå®‰è£…")
                self.results['issues'].append(f"ç¼ºå°‘ä¾èµ–åŒ…: {package}")
        
        self.results['dependencies'] = dependencies
    
    def check_gpu_status(self):
        """æ£€æŸ¥GPUçŠ¶æ€"""
        print("\nğŸ® æ£€æŸ¥GPUçŠ¶æ€...")
        
        gpu_info = {
            'cuda_available': False,
            'gpu_count': 0,
            'gpu_devices': []
        }
        
        if TORCH_AVAILABLE:
            try:
                gpu_info['cuda_available'] = torch.cuda.is_available()
                gpu_info['gpu_count'] = torch.cuda.device_count()
                
                for i in range(torch.cuda.device_count()):
                    props = torch.cuda.get_device_properties(i)
                    gpu_device = {
                        'id': i,
                        'name': props.name,
                        'memory': f"{props.total_memory / (1024**3):.1f} GB",
                        'compute_capability': f"{props.major}.{props.minor}"
                    }
                    gpu_info['gpu_devices'].append(gpu_device)
                    
                print(f"âœ… CUDAå¯ç”¨: {gpu_info['cuda_available']}")
                print(f"âœ… GPUæ•°é‡: {gpu_info['gpu_count']}")
                
                for device in gpu_info['gpu_devices']:
                    print(f"  GPU {device['id']}: {device['name']} ({device['memory']})")
                
            except Exception as e:
                print(f"âŒ GPUæ£€æŸ¥å¤±è´¥: {str(e)}")
                self.results['issues'].append(f"GPUæ£€æŸ¥å¤±è´¥: {str(e)}")
        else:
            print("âŒ PyTorchæœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥GPU")
            self.results['issues'].append("PyTorchæœªå®‰è£…")
        
        # æ£€æŸ¥NVIDIAé©±åŠ¨
        try:
            result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                print("âœ… NVIDIAé©±åŠ¨æ­£å¸¸")
                gpu_info['nvidia_driver'] = True
            else:
                print("âŒ NVIDIAé©±åŠ¨é—®é¢˜")
                gpu_info['nvidia_driver'] = False
        except (FileNotFoundError, subprocess.TimeoutExpired):
            print("âŒ NVIDIAé©±åŠ¨æœªå®‰è£…æˆ–ä¸å¯ç”¨")
            gpu_info['nvidia_driver'] = False
            self.results['issues'].append("NVIDIAé©±åŠ¨æœªå®‰è£…")
        
        self.results['gpu_status'] = gpu_info
    
    def check_ports(self):
        """æ£€æŸ¥ç«¯å£å ç”¨"""
        print("\nğŸ”Œ æ£€æŸ¥ç«¯å£å ç”¨...")
        
        ports_to_check = {
            8888: 'Jupyter Lab',
            5173: 'Webç•Œé¢ (å¼€å‘)',
            6006: 'TensorBoard'
        }
        
        port_status = {}
        
        for port, description in ports_to_check.items():
            try:
                result = subprocess.run(
                    ['netstat', '-ano'], 
                    capture_output=True, 
                    text=True,
                    timeout=5
                )
                
                if f':{port}' in result.stdout:
                    port_status[port] = {
                        'status': 'å ç”¨',
                        'description': description,
                        'available': False
                    }
                    print(f"âš ï¸  ç«¯å£ {port} ({description}) è¢«å ç”¨")
                else:
                    port_status[port] = {
                        'status': 'å¯ç”¨',
                        'description': description,
                        'available': True
                    }
                    print(f"âœ… ç«¯å£ {port} ({description}) å¯ç”¨")
                    
            except Exception as e:
                port_status[port] = {
                    'status': 'æ£€æŸ¥å¤±è´¥',
                    'description': description,
                    'available': None,
                    'error': str(e)
                }
                print(f"âŒ ç«¯å£ {port} æ£€æŸ¥å¤±è´¥: {str(e)}")
        
        self.results['ports'] = port_status
    
    def run_performance_tests(self):
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        print("\nâš¡ è¿è¡Œæ€§èƒ½æµ‹è¯•...")
        
        performance = {}
        
        # CPUæ€§èƒ½æµ‹è¯•
        print("  æµ‹è¯•CPUè®¡ç®—æ€§èƒ½...")
        start_time = time.time()
        try:
            # NumPyè®¡ç®—æµ‹è¯•
            if NUMPY_AVAILABLE:
                x = np.random.randn(1000, 1000)
                y = np.random.randn(1000, 1000)
                _ = np.dot(x, y)
                numpy_time = time.time() - start_time
                performance['numpy_matrix_multiply'] = f"{numpy_time:.4f}s"
                print(f"    NumPyçŸ©é˜µä¹˜æ³•: {numpy_time:.4f}s")
            
            # Pythonå¾ªç¯æµ‹è¯•
            start_time = time.time()
            result = sum(range(1000000))
            python_loop_time = time.time() - start_time
            performance['python_loop'] = f"{python_loop_time:.4f}s"
            print(f"    Pythonå¾ªç¯(1Mæ¬¡): {python_loop_time:.4f}s")
            
        except Exception as e:
            performance['cpu_test_error'] = str(e)
            print(f"  âŒ CPUæµ‹è¯•å¤±è´¥: {str(e)}")
        
        # GPUæ€§èƒ½æµ‹è¯•
        if TORCH_AVAILABLE and torch.cuda.is_available():
            print("  æµ‹è¯•GPUè®¡ç®—æ€§èƒ½...")
            try:
                device = torch.device('cuda')
                x = torch.randn(1000, 1000, device=device)
                y = torch.randn(1000, 1000, device=device)
                
                # é¢„çƒ­
                for _ in range(3):
                    _ = torch.mm(x, y)
                
                # æ­£å¼æµ‹è¯•
                start_time = time.time()
                result = torch.mm(x, y)
                gpu_time = time.time() - start_time
                
                performance['gpu_matrix_multiply'] = f"{gpu_time:.4f}s"
                print(f"    GPUçŸ©é˜µä¹˜æ³•: {gpu_time:.4f}s")
                
                # è®¡ç®—åŠ é€Ÿæ¯”
                if 'numpy_matrix_multiply' in performance:
                    speedup = float(performance['numpy_matrix_multiply'].replace('s', '')) / gpu_time
                    performance['gpu_speedup'] = f"{speedup:.1f}x"
                    print(f"    GPUåŠ é€Ÿæ¯”: {speedup:.1f}x")
                
            except Exception as e:
                performance['gpu_test_error'] = str(e)
                print(f"  âŒ GPUæµ‹è¯•å¤±è´¥: {str(e)}")
        else:
            print("  è·³è¿‡GPUæµ‹è¯• (CUDAä¸å¯ç”¨)")
        
        # å†…å­˜æµ‹è¯•
        try:
            memory = psutil.virtual_memory()
            performance['memory_usage'] = {
                'total': f"{memory.total / (1024**3):.1f} GB",
                'used': f"{memory.used / (1024**3):.1f} GB",
                'available': f"{memory.available / (1024**3):.1f} GB",
                'percent': f"{memory.percent}%"
            }
            print(f"  å†…å­˜ä½¿ç”¨ç‡: {memory.percent}%")
            
        except Exception as e:
            performance['memory_test_error'] = str(e)
            print(f"  âŒ å†…å­˜æµ‹è¯•å¤±è´¥: {str(e)}")
        
        self.results['performance'] = performance
    
    def check_project_structure(self):
        """æ£€æŸ¥é¡¹ç›®ç»“æ„"""
        print("\nğŸ“ æ£€æŸ¥é¡¹ç›®ç»“æ„...")
        
        project_files = [
            'requirements.txt',
            'setup.py',
            'cli_demo.py',
            'src/',
            'demos/',
            'ui/brain-ai-ui/'
        ]
        
        project_structure = {}
        
        for file_path in project_files:
            exists = os.path.exists(file_path)
            project_structure[file_path] = exists
            status = "âœ…" if exists else "âŒ"
            print(f"{status} {file_path}")
        
        self.results['project_structure'] = project_structure
    
    def generate_recommendations(self):
        """ç”Ÿæˆå»ºè®®"""
        print("\nğŸ’¡ ç”Ÿæˆå»ºè®®...")
        
        recommendations = []
        
        # æ ¹æ®æ£€æŸ¥ç»“æœç”Ÿæˆå»ºè®®
        if not TORCH_AVAILABLE:
            recommendations.append("å®‰è£…PyTorch: pip install torch torchvision torchaudio")
        
        gpu_status = self.results.get('gpu_status', {})
        if not gpu_status.get('cuda_available', False) and gpu_status.get('nvidia_driver', False):
            recommendations.append("å®‰è£…CUDAç‰ˆæœ¬çš„PyTorchä»¥å¯ç”¨GPUåŠ é€Ÿ")
        
        if self.results['issues']:
            recommendations.append("è§£å†³ä¸Šè¿°æ ‡è®°çš„é—®é¢˜ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        
        # æ€§èƒ½å»ºè®®
        performance = self.results.get('performance', {})
        if 'gpu_speedup' in performance:
            speedup = float(performance['gpu_speedup'].replace('x', ''))
            if speedup > 5:
                recommendations.append("GPUåŠ é€Ÿæ•ˆæœè‰¯å¥½ï¼Œå»ºè®®å……åˆ†åˆ©ç”¨GPUè¿›è¡Œè®­ç»ƒ")
        
        # å†…å­˜å»ºè®®
        memory_info = performance.get('memory_usage', {})
        if memory_info.get('percent'):
            usage_percent = int(memory_info['percent'].replace('%', ''))
            if usage_percent > 80:
                recommendations.append("å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®å…³é—­ä¸å¿…è¦çš„ç¨‹åºæˆ–å¢åŠ å†…å­˜")
        
        recommendations.extend([
            "å®šæœŸæ›´æ–°GPUé©±åŠ¨ç¨‹åºä»¥è·å¾—æœ€ä½³æ€§èƒ½",
            "ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒéš”ç¦»é¡¹ç›®ä¾èµ–",
            "è€ƒè™‘ä½¿ç”¨SSDå­˜å‚¨ä»¥æé«˜I/Oæ€§èƒ½",
            "å®šæœŸæ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç¼“å­˜"
        ])
        
        self.results['recommendations'] = recommendations
        
        print("å»ºè®®ç”Ÿæˆå®Œæˆ")
    
    def save_report(self, filename='diagnosis_report.json'):
        """ä¿å­˜è¯Šæ–­æŠ¥å‘Š"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ“„ è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}")
    
    def print_summary(self):
        """æ‰“å°æ€»ç»“"""
        print("\n" + "="*60)
        print("ğŸ“Š è¯Šæ–­æ€»ç»“")
        print("="*60)
        
        # ç³»ç»ŸçŠ¶æ€
        print(f"ğŸ–¥ï¸  ç³»ç»Ÿ: {self.results['system_info'].get('os', 'Unknown')} {self.results['system_info'].get('os_version', 'Unknown')}")
        print(f"ğŸ§  Python: {platform.python_version()}")
        print(f"ğŸ’¾ å†…å­˜: {self.results['system_info'].get('ram_total', 'Unknown')}")
        
        # ä¾èµ–çŠ¶æ€
        deps = self.results.get('dependencies', {})
        installed = sum(1 for v in deps.values() if v.get('installed', False))
        total = len(deps)
        print(f"ğŸ“¦ ä¾èµ–: {installed}/{total} å·²å®‰è£…")
        
        # GPUçŠ¶æ€
        gpu_info = self.results.get('gpu_status', {})
        print(f"ğŸ® GPU: {'å¯ç”¨' if gpu_info.get('cuda_available', False) else 'ä¸å¯ç”¨'}")
        
        # é—®é¢˜ç»Ÿè®¡
        issues = self.results.get('issues', [])
        if issues:
            print(f"âš ï¸  é—®é¢˜: {len(issues)} ä¸ªé—®é¢˜éœ€è¦è§£å†³")
        else:
            print("âœ… æ²¡æœ‰å‘ç°é—®é¢˜")
        
        # æ€§èƒ½æŒ‡æ ‡
        performance = self.results.get('performance', {})
        if 'gpu_speedup' in performance:
            print(f"âš¡ æ€§èƒ½: GPUåŠ é€Ÿ {performance['gpu_speedup']}")
        
        print("\n" + "="*60)
    
    def run_full_diagnosis(self):
        """è¿è¡Œå®Œæ•´è¯Šæ–­"""
        print("ğŸ§  Brain AI Windows 11 ç¯å¢ƒè¯Šæ–­")
        print("="*60)
        print(f"è¯Šæ–­æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        
        try:
            self.check_system_info()
            self.check_python_environment()
            self.check_dependencies()
            self.check_gpu_status()
            self.check_ports()
            self.run_performance_tests()
            self.check_project_structure()
            self.generate_recommendations()
            
            self.print_summary()
            self.save_report()
            
            # æ‰“å°å»ºè®®
            print("\nğŸ’¡ å»ºè®®:")
            for i, rec in enumerate(self.results['recommendations'], 1):
                print(f"{i}. {rec}")
            
            print("\n" + "="*60)
            print("âœ… è¯Šæ–­å®Œæˆï¼è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜ã€‚")
            
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸  è¯Šæ–­è¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            print(f"\nâŒ è¯Šæ–­è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()

def main():
    """ä¸»å‡½æ•°"""
    diagnosis = BrainAIDiagnosis()
    diagnosis.run_full_diagnosis()

if __name__ == "__main__":
    main()