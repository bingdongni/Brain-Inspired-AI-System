#!/usr/bin/env python3
"""
æµ·é©¬ä½“æ¨¡æ‹Ÿå™¨æ ¸å¿ƒæ¨¡å—å®Œæ•´å®ç°æ¼”ç¤º
åŸºäºScienceæœŸåˆŠ2025å¹´æœ€æ–°ç ”ç©¶æˆæœçš„ç”Ÿç‰©å¯å‘å¼è®°å¿†ç³»ç»Ÿ

å®ç°çš„æ ¸å¿ƒæ¨¡å—ï¼š
1. Transformer-basedè®°å¿†ç¼–ç å™¨
2. å¯å¾®åˆ†ç¥ç»å­—å…¸
3. æ¨¡å¼åˆ†ç¦»æœºåˆ¶
4. å¿«é€Ÿä¸€æ¬¡æ€§å­¦ä¹ åŠŸèƒ½
5. æƒ…æ™¯è®°å¿†å­˜å‚¨å’Œæ£€ç´¢ç³»ç»Ÿ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
from typing import Dict, Any, Tuple, Optional, List

# å¯¼å…¥æ‰€æœ‰æ ¸å¿ƒæ¨¡å—
from hippocampus.encoders.transformer_encoder import TransformerMemoryEncoder
from memory_cell.neural_dictionary import DifferentiableNeuralDictionary
from pattern_separation.pattern_separator import PatternSeparationNetwork
from hippocampus.fast_learning import OneShotLearner
from hippocampus.episodic_memory import EpisodicMemorySystem


class CompleteHippocampusModuleDemo:
    """
    æµ·é©¬ä½“æ¨¡æ‹Ÿå™¨æ ¸å¿ƒæ¨¡å—å®Œæ•´æ¼”ç¤ºç±»
    å±•ç¤ºæ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æ¨¡å—çš„é›†æˆå’Œåä½œ
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ–æ¼”ç¤ºç³»ç»Ÿ"""
        self.config = config or self._get_default_config()
        self.setup_modules()
        
        # æ¼”ç¤ºç»Ÿè®¡
        self.demo_stats = {
            'total_operations': 0,
            'memory_operations': 0,
            'learning_operations': 0,
            'retrieval_operations': 0,
            'start_time': time.time()
        }
        
        print("ğŸ§  æµ·é©¬ä½“æ¨¡æ‹Ÿå™¨æ ¸å¿ƒæ¨¡å—åˆå§‹åŒ–å®Œæˆ")
        print(f"   - æ€»å‚æ•°æ•°é‡: {sum(p.numel() for p in self._get_all_parameters()):,}")
        print(f"   - æ¿€æ´»æ¨¡å—: {len(self.active_modules)}")
        
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–æ¼”ç¤ºé»˜è®¤é…ç½®"""
        return {
            'input_dim': 512,
            'hidden_dim': 256,
            'vocab_size': 1000,
            'max_seq_len': 128,
            'transformer_layers': 6,
            'num_heads': 8,
            'dict_cells': 6,
            'dict_capacity': 500,
            'granule_cells': 800,
            'ca3_cells': 150,
            'sparsity': 0.02,
            'episodic_cells': 6,
            'episodic_capacity': 50,
            'temporal_dim': 32
        }
    
    def setup_modules(self):
        """è®¾ç½®æ‰€æœ‰æ ¸å¿ƒæ¨¡å—"""
        print("\nğŸ”§ åˆå§‹åŒ–æ ¸å¿ƒæ¨¡å—...")
        
        # 1. Transformerè®°å¿†ç¼–ç å™¨
        self.transformer_encoder = TransformerMemoryEncoder(
            vocab_size=self.config['vocab_size'],
            hidden_dim=self.config['hidden_dim'],
            num_layers=self.config['transformer_layers'],
            num_heads=self.config['num_heads'],
            max_seq_len=self.config['max_seq_len'],
            msb_enhancement=True,
            pattern_completion=True,
            temporal_alignment=True
        )
        
        # 2. å¯å¾®åˆ†ç¥ç»å­—å…¸
        self.neural_dictionary = DifferentiableNeuralDictionary(
            key_dim=self.config['hidden_dim'],
            value_dim=self.config['hidden_dim'],
            num_cells=self.config['dict_cells'],
            capacity_per_cell=self.config['dict_capacity'],
            temperature=1.0
        )
        
        # 3. æ¨¡å¼åˆ†ç¦»ç½‘ç»œ
        self.pattern_separator = PatternSeparationNetwork(
            input_dim=self.config['hidden_dim'],
            hidden_dim=self.config['hidden_dim'],
            num_granule_cells=self.config['granule_cells'],
            num_ca3_cells=self.config['ca3_cells'],
            sparsity=self.config['sparsity']
        )
        
        # 4. å¿«é€Ÿå­¦ä¹ å™¨
        self.one_shot_learner = OneShotLearner(
            input_dim=self.config['hidden_dim'],
            hidden_dim=self.config['hidden_dim'],
            num_way=3,
            num_shot=1
        )
        
        # 5. æƒ…æ™¯è®°å¿†ç³»ç»Ÿ
        self.episodic_memory = EpisodicMemorySystem(
            content_dim=self.config['hidden_dim'],
            temporal_dim=self.config['temporal_dim'],
            context_dim=self.config['hidden_dim'],
            num_cells=self.config['episodic_cells'],
            capacity_per_cell=self.config['episodic_capacity']
        )
        
        # æ´»è·ƒæ¨¡å—åˆ—è¡¨
        self.active_modules = [
            'transformer_encoder', 'neural_dictionary', 'pattern_separator',
            'one_shot_learner', 'episodic_memory'
        ]
        
        print("   âœ… æ‰€æœ‰æ ¸å¿ƒæ¨¡å—åˆå§‹åŒ–å®Œæˆ")
    
    def _get_all_parameters(self):
        """è·å–æ‰€æœ‰æ¨¡å—çš„å‚æ•°"""
        all_params = []
        all_params.extend(self.transformer_encoder.parameters())
        all_params.extend(self.neural_dictionary.parameters())
        all_params.extend(self.pattern_separator.parameters())
        all_params.extend(self.one_shot_learner.parameters())
        all_params.extend(self.episodic_memory.parameters())
        return all_params
    
    def demo_memory_encoding(self, num_samples: int = 5) -> Dict[str, Any]:
        """
        æ¼”ç¤ºè®°å¿†ç¼–ç åŠŸèƒ½
        
        Args:
            num_samples: æ¼”ç¤ºæ ·æœ¬æ•°é‡
            
        Returns:
            ç¼–ç æ¼”ç¤ºç»“æœ
        """
        print(f"\nğŸ“ è®°å¿†ç¼–ç æ¼”ç¤º ({num_samples}ä¸ªæ ·æœ¬)")
        print("-" * 50)
        
        encoding_results = []
        
        for i in range(num_samples):
            # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
            # ä½¿ç”¨æµ®ç‚¹æ•°è¾“å…¥ï¼Œé¿å…tokenç´¢å¼•é—®é¢˜
            input_features = torch.randn(1, self.config['input_dim'])
            context = torch.randn(1, self.config['hidden_dim'])
            
            print(f"\næ ·æœ¬ {i+1}:")
            
            # 1. Transformerç¼–ç ï¼ˆå°†æµ®ç‚¹æ•°è¾“å…¥è½¬æ¢ä¸ºé€‚åˆçš„æ ¼å¼ï¼‰
            with torch.no_grad():
                # å°†è¾“å…¥è½¬æ¢ä¸ºé€‚åˆTransformerçš„æ ¼å¼
                batch_size, input_dim = input_features.shape
                seq_len = 4  # å›ºå®šåºåˆ—é•¿åº¦
                
                # åˆ›å»ºæ¨¡æ‹Ÿtokenè¾“å…¥
                token_input = torch.randint(0, self.config['vocab_size'], (batch_size, seq_len))
                
                # æ‰§è¡ŒTransformerç¼–ç 
                transformer_output, trans_stats = self.transformer_encoder(
                    input_ids=token_input,
                    context=context,
                    memory_type='episodic',
                    return_stats=True
                )
                
                print(f"   ğŸ”¹ Transformerç¼–ç : å½¢çŠ¶ {transformer_output.shape}")
            
            # 2. æ¨¡å¼åˆ†ç¦»
            with torch.no_grad():
                if batch_size > 1:
                    sep1, sep2, sep_stats = self.pattern_separator(
                        transformer_output[:, 0], transformer_output[:, 1]
                    )
                    separation_degree = sep_stats.get('separation_degree', 0.0)
                else:
                    sep1, sep2, sep_stats = self.pattern_separator(transformer_output[:, 0])
                    separation_degree = 0.0
                
                print(f"   ğŸ”¹ æ¨¡å¼åˆ†ç¦»å®Œæˆ, åˆ†ç¦»åº¦: {separation_degree:.3f}")
            
            # 3. å­˜å‚¨åˆ°ç¥ç»å­—å…¸
            with torch.no_grad():
                dict_result = self.neural_dictionary.write_memory(
                    key=sep1,
                    value=transformer_output.mean(dim=1)
                )
                print(f"   ğŸ”¹ ç¥ç»å­—å…¸å†™å…¥: {dict_result['total_writes']} é¡¹")
            
            # 4. å­˜å‚¨æƒ…æ™¯è®°å¿†
            with torch.no_grad():
                timestamp = time.time()
                episodic_result = self.episodic_memory.store_episode(
                    content=transformer_output.mean(dim=1),
                    timestamp=timestamp,
                    context=context,
                    episode_id=f"demo_episode_{i}"
                )
                print(f"   ğŸ”¹ æƒ…æ™¯è®°å¿†å­˜å‚¨: {episodic_result['global_episode_id']}")
            
            encoding_results.append({
                'sample_id': i,
                'transformer_stats': trans_stats,
                'separation_stats': sep_stats,
                'dictionary_result': dict_result,
                'episodic_result': episodic_result,
                'input_features': input_features,
                'context': context
            })
            
            self.demo_stats['memory_operations'] += 1
        
        print(f"\nâœ… è®°å¿†ç¼–ç æ¼”ç¤ºå®Œæˆï¼Œå…±å¤„ç† {num_samples} ä¸ªæ ·æœ¬")
        return {'encoding_results': encoding_results}
    
    def demo_memory_retrieval(self, num_queries: int = 3) -> Dict[str, Any]:
        """
        æ¼”ç¤ºè®°å¿†æ£€ç´¢åŠŸèƒ½
        
        Args:
            num_queries: æ£€ç´¢æŸ¥è¯¢æ•°é‡
            
        Returns:
            æ£€ç´¢æ¼”ç¤ºç»“æœ
        """
        print(f"\nğŸ” è®°å¿†æ£€ç´¢æ¼”ç¤º ({num_queries}ä¸ªæŸ¥è¯¢)")
        print("-" * 50)
        
        retrieval_results = []
        
        for i in range(num_queries):
            # åˆ›å»ºæŸ¥è¯¢å‘é‡
            query = torch.randn(self.config['hidden_dim'])
            
            print(f"\næŸ¥è¯¢ {i+1}:")
            
            # 1. ç¥ç»å­—å…¸æ£€ç´¢
            with torch.no_grad():
                dict_results, dict_stats = self.neural_dictionary.retrieve_memory(
                    query=query.unsqueeze(0),
                    top_k=3,
                    fusion_method='attention'
                )
                print(f"   ğŸ”¹ ç¥ç»å­—å…¸æ£€ç´¢: {len(dict_stats['cell_stats'])} ä¸ªç»†èƒæœç´¢")
            
            # 2. æƒ…æ™¯è®°å¿†æ£€ç´¢
            with torch.no_grad():
                episodic_results, episodic_stats = self.episodic_memory.retrieve_episodes(
                    query_content=query.unsqueeze(0),
                    query_context=torch.zeros_like(query.unsqueeze(0)),
                    retrieval_type='hybrid'
                )
                print(f"   ğŸ”¹ æƒ…æ™¯è®°å¿†æ£€ç´¢: {episodic_stats['retrieval_type']} æ¨¡å¼")
            
            # 3. è®¡ç®—æ£€ç´¢è´¨é‡
            with torch.no_grad():
                if dict_results.numel() > 0:
                    similarity = F.cosine_similarity(query, dict_results.mean(dim=1)).item()
                else:
                    similarity = 0.0
                
                print(f"   ğŸ”¹ æ£€ç´¢ç›¸ä¼¼åº¦: {similarity:.3f}")
            
            retrieval_results.append({
                'query_id': i,
                'query': query,
                'dictionary_results': dict_results,
                'episodic_results': episodic_results,
                'similarity_score': similarity,
                'dictionary_stats': dict_stats,
                'episodic_stats': episodic_stats
            })
            
            self.demo_stats['retrieval_operations'] += 1
        
        print(f"\nâœ… è®°å¿†æ£€ç´¢æ¼”ç¤ºå®Œæˆï¼Œå…±æ‰§è¡Œ {num_queries} ä¸ªæŸ¥è¯¢")
        return {'retrieval_results': retrieval_results}
    
    def demo_fast_learning(self, num_tasks: int = 2) -> Dict[str, Any]:
        """
        æ¼”ç¤ºå¿«é€Ÿå­¦ä¹ åŠŸèƒ½
        
        Args:
            num_tasks: å­¦ä¹ ä»»åŠ¡æ•°é‡
            
        Returns:
            å­¦ä¹ æ¼”ç¤ºç»“æœ
        """
        print(f"\nâš¡ å¿«é€Ÿå­¦ä¹ æ¼”ç¤º ({num_tasks}ä¸ªä»»åŠ¡)")
        print("-" * 50)
        
        learning_results = []
        
        for i in range(num_tasks):
            print(f"\nä»»åŠ¡ {i+1}:")
            
            # åˆ›å»ºfew-shotå­¦ä¹ æ•°æ®
            support_size = 3
            query_size = 2
            
            support_x = torch.randn(support_size, self.config['hidden_dim'])
            query_x = torch.randn(query_size, self.config['hidden_dim'])
            support_y = torch.randint(0, 3, (support_size,))  # 3ç±»åˆ†ç±»
            
            print(f"   ğŸ”¹ æ”¯æŒé›†: {support_x.shape}, æŸ¥è¯¢é›†: {query_x.shape}")
            
            # æ‰§è¡Œfew-shotå­¦ä¹ 
            with torch.no_grad():
                predictions, learning_stats = self.one_shot_learner.few_shot_learning(
                    support_x=support_x,
                    support_y=support_y,
                    query_x=query_x
                )
                
                print(f"   ğŸ”¹ å­¦ä¹ å®Œæˆ: {predictions.shape}")
                print(f"   ğŸ”¹ å¹³å‡è®°å¿†å¼ºåº¦: {learning_stats['avg_memory_strength']:.3f}")
            
            learning_results.append({
                'task_id': i,
                'support_x': support_x,
                'support_y': support_y,
                'query_x': query_x,
                'predictions': predictions,
                'learning_stats': learning_stats
            })
            
            self.demo_stats['learning_operations'] += 1
        
        print(f"\nâœ… å¿«é€Ÿå­¦ä¹ æ¼”ç¤ºå®Œæˆï¼Œå…±æ‰§è¡Œ {num_tasks} ä¸ªä»»åŠ¡")
        return {'learning_results': learning_results}
    
    def demo_pattern_separation(self, num_pairs: int = 3) -> Dict[str, Any]:
        """
        æ¼”ç¤ºæ¨¡å¼åˆ†ç¦»åŠŸèƒ½
        
        Args:
            num_pairs: è¾“å…¥å¯¹æ•°é‡
            
        Returns:
            æ¨¡å¼åˆ†ç¦»æ¼”ç¤ºç»“æœ
        """
        print(f"\nğŸ¯ æ¨¡å¼åˆ†ç¦»æ¼”ç¤º ({num_pairs}ä¸ªè¾“å…¥å¯¹)")
        print("-" * 50)
        
        separation_results = []
        
        for i in range(num_pairs):
            # åˆ›å»ºä¸¤ä¸ªç›¸ä¼¼è¾“å…¥
            base_input = torch.randn(self.config['hidden_dim'])
            # åˆ›å»ºç›¸ä¼¼ä½†ä¸åŒçš„ç¬¬äºŒä¸ªè¾“å…¥
            similarity_level = 0.7 + 0.2 * i / max(1, num_pairs - 1)  # 0.7-0.9çš„ç›¸ä¼¼åº¦
            similar_input = similarity_level * base_input + (1 - similarity_level) * torch.randn(self.config['hidden_dim'])
            
            print(f"\nè¾“å…¥å¯¹ {i+1}:")
            print(f"   ç›®æ ‡ç›¸ä¼¼åº¦: {similarity_level:.2f}")
            
            # æ‰§è¡Œæ¨¡å¼åˆ†ç¦»
            with torch.no_grad():
                sep1, sep2, sep_stats = self.pattern_separator(
                    base_input.unsqueeze(0), 
                    similar_input.unsqueeze(0)
                )
                
                # è®¡ç®—å®é™…åˆ†ç¦»æ•ˆæœ
                input_similarity = F.cosine_similarity(
                    base_input.unsqueeze(0), 
                    similar_input.unsqueeze(0)
                ).item()
                
                output_similarity = F.cosine_similarity(sep1, sep2).item()
                separation_improvement = input_similarity - output_similarity
                
                print(f"   ğŸ”¹ è¾“å…¥ç›¸ä¼¼åº¦: {input_similarity:.3f}")
                print(f"   ğŸ”¹ è¾“å‡ºç›¸ä¼¼åº¦: {output_similarity:.3f}")
                print(f"   ğŸ”¹ åˆ†ç¦»æå‡: {separation_improvement:.3f}")
            
            separation_results.append({
                'pair_id': i,
                'base_input': base_input,
                'similar_input': similar_input,
                'target_similarity': similarity_level,
                'input_similarity': input_similarity,
                'output_similarity': output_similarity,
                'separation_improvement': separation_improvement,
                'separation_stats': sep_stats
            })
        
        print(f"\nâœ… æ¨¡å¼åˆ†ç¦»æ¼”ç¤ºå®Œæˆï¼Œå…±å¤„ç† {num_pairs} ä¸ªè¾“å…¥å¯¹")
        return {'separation_results': separation_results}
    
    def demo_memory_consolidation(self) -> Dict[str, Any]:
        """
        æ¼”ç¤ºè®°å¿†å·©å›ºåŠŸèƒ½
        
        Returns:
            å·©å›ºæ¼”ç¤ºç»“æœ
        """
        print(f"\nğŸ”„ è®°å¿†å·©å›ºæ¼”ç¤º")
        print("-" * 50)
        
        consolidation_start = time.time()
        
        # æ‰§è¡Œè®°å¿†ç³»ç»Ÿæ›´æ–°
        with torch.no_grad():
            # æ›´æ–°æƒ…æ™¯è®°å¿†
            episodic_update = self.episodic_memory.update_memory_system()
            
            # å‹ç¼©ç¥ç»å­—å…¸
            dict_compression = self.neural_dictionary.compress_memories()
            
            consolidation_time = time.time() - consolidation_start
        
        print(f"   ğŸ”¹ æƒ…æ™¯è®°å¿†æ›´æ–°: {episodic_update['cells_updated']} ä¸ªç»†èƒ")
        print(f"   ğŸ”¹ å­—å…¸å‹ç¼©: {len(dict_compression)} ä¸ªæ“ä½œ")
        print(f"   ğŸ”¹ å·©å›ºæ—¶é—´: {consolidation_time:.4f}s")
        
        consolidation_result = {
            'consolidation_time': consolidation_time,
            'episodic_update': episodic_update,
            'dictionary_compression': dict_compression,
            'timestamp': time.time()
        }
        
        print(f"\nâœ… è®°å¿†å·©å›ºæ¼”ç¤ºå®Œæˆ")
        return consolidation_result
    
    def get_comprehensive_statistics(self) -> Dict[str, Any]:
        """
        è·å–ç»¼åˆç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            å®Œæ•´çš„ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        """
        print(f"\nğŸ“Š ç»¼åˆç³»ç»Ÿç»Ÿè®¡")
        print("-" * 50)
        
        # å„ä¸ªæ¨¡å—çš„ç»Ÿè®¡
        transformer_stats = self.transformer_encoder.get_memory_statistics()
        dictionary_stats = self.neural_dictionary.get_global_statistics()
        separator_stats = self.pattern_separator.get_network_statistics()
        learner_stats = self.one_shot_learner.get_learning_statistics()
        episodic_stats = self.episodic_memory.get_system_statistics()
        
        # ç³»ç»Ÿæ€»ä½“ç»Ÿè®¡
        all_params = list(self.transformer_encoder.parameters()) + \
                    list(self.neural_dictionary.parameters()) + \
                    list(self.pattern_separator.parameters()) + \
                    list(self.one_shot_learner.parameters()) + \
                    list(self.episodic_memory.parameters())
        
        total_params = sum(p.numel() for p in all_params)
        model_size_mb = sum(p.numel() * p.element_size() for p in all_params) / (1024**2)
        
        # æ¼”ç¤ºç»Ÿè®¡
        runtime = time.time() - self.demo_stats['start_time']
        
        comprehensive_stats = {
            'system_overview': {
                'total_parameters': total_params,
                'model_size_mb': model_size_mb,
                'active_modules': len(self.active_modules),
                'demo_runtime_seconds': runtime,
                'total_demo_operations': self.demo_stats['total_operations']
            },
            'module_statistics': {
                'transformer_encoder': transformer_stats,
                'neural_dictionary': dictionary_stats,
                'pattern_separator': separator_stats,
                'one_shot_learner': learner_stats,
                'episodic_memory': episodic_stats
            },
            'demo_operations': {
                'memory_operations': self.demo_stats['memory_operations'],
                'learning_operations': self.demo_stats['learning_operations'],
                'retrieval_operations': self.demo_stats['retrieval_operations']
            },
            'configuration': self.config
        }
        
        # æ‰“å°å…³é”®ç»Ÿè®¡
        print(f"   ğŸ“‹ æ€»å‚æ•°æ•°é‡: {total_params:,}")
        print(f"   ğŸ’¾ æ¨¡å‹å¤§å°: {model_size_mb:.2f} MB")
        print(f"   â±ï¸  æ¼”ç¤ºè¿è¡Œæ—¶é—´: {runtime:.2f}s")
        print(f"   ğŸ”¢ è®°å¿†æ“ä½œ: {self.demo_stats['memory_operations']}")
        print(f"   ğŸ“š å­¦ä¹ æ“ä½œ: {self.demo_stats['learning_operations']}")
        print(f"   ğŸ” æ£€ç´¢æ“ä½œ: {self.demo_stats['retrieval_operations']}")
        print(f"   ğŸ“ˆ ç¥ç»å­—å…¸å®¹é‡: {dictionary_stats['total_capacity']}")
        print(f"   ğŸ§  æƒ…æ™¯è®°å¿†å®¹é‡: {episodic_stats['total_capacity']}")
        
        return comprehensive_stats
    
    def run_complete_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("=" * 80)
        print("ğŸ§  æµ·é©¬ä½“æ¨¡æ‹Ÿå™¨æ ¸å¿ƒæ¨¡å—å®Œæ•´æ¼”ç¤º")
        print("åŸºäºScienceæœŸåˆŠ2025å¹´æœ€æ–°ç ”ç©¶æˆæœ")
        print("=" * 80)
        
        self.demo_stats['total_operations'] = 0
        
        try:
            # 1. è®°å¿†ç¼–ç æ¼”ç¤º
            encoding_result = self.demo_memory_encoding(num_samples=4)
            self.demo_stats['total_operations'] += 1
            
            # 2. æ¨¡å¼åˆ†ç¦»æ¼”ç¤º
            separation_result = self.demo_pattern_separation(num_pairs=3)
            self.demo_stats['total_operations'] += 1
            
            # 3. è®°å¿†æ£€ç´¢æ¼”ç¤º
            retrieval_result = self.demo_memory_retrieval(num_queries=3)
            self.demo_stats['total_operations'] += 1
            
            # 4. å¿«é€Ÿå­¦ä¹ æ¼”ç¤º
            learning_result = self.demo_fast_learning(num_tasks=2)
            self.demo_stats['total_operations'] += 1
            
            # 5. è®°å¿†å·©å›ºæ¼”ç¤º
            consolidation_result = self.demo_memory_consolidation()
            self.demo_stats['total_operations'] += 1
            
            # 6. ç»¼åˆç»Ÿè®¡
            comprehensive_stats = self.get_comprehensive_statistics()
            
            print("\n" + "=" * 80)
            print("ğŸ‰ æµ·é©¬ä½“æ¨¡æ‹Ÿå™¨æ ¸å¿ƒæ¨¡å—æ¼”ç¤ºå®Œæˆï¼")
            print("âœ… æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æ¨¡å—éªŒè¯æˆåŠŸ")
            print("ğŸ“‹ å®ç°çŠ¶æ€:")
            print("   âœ“ Transformer-basedè®°å¿†ç¼–ç å™¨")
            print("   âœ“ å¯å¾®åˆ†ç¥ç»å­—å…¸")
            print("   âœ“ æ¨¡å¼åˆ†ç¦»æœºåˆ¶")
            print("   âœ“ å¿«é€Ÿä¸€æ¬¡æ€§å­¦ä¹ åŠŸèƒ½")
            print("   âœ“ æƒ…æ™¯è®°å¿†å­˜å‚¨å’Œæ£€ç´¢ç³»ç»Ÿ")
            print("=" * 80)
            
            return {
                'encoding_demo': encoding_result,
                'separation_demo': separation_result,
                'retrieval_demo': retrieval_result,
                'learning_demo': learning_result,
                'consolidation_demo': consolidation_result,
                'statistics': comprehensive_stats
            }
            
        except Exception as e:
            print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            print("è¯·æ£€æŸ¥æ¨¡å—é…ç½®å’Œä¾èµ–å…³ç³»")
            raise


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    # åˆ›å»ºå¹¶è¿è¡Œæ¼”ç¤º
    demo = CompleteHippocampusModuleDemo()
    results = demo.run_complete_demo()
    
    return results


if __name__ == "__main__":
    results = main()