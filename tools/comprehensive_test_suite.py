#!/usr/bin/env python3
"""
å…¨é¢ç³»ç»Ÿæµ‹è¯•éªŒè¯å¥—ä»¶
Comprehensive System Testing and Validation Suite

åŒ…æ‹¬ï¼š
1. åŸºå‡†æµ‹è¯•å¥—ä»¶ - æ ‡å‡†åŒ–çš„æµ‹è¯•æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡
2. æŒç»­å­¦ä¹ èƒ½åŠ›éªŒè¯ - ç¾éš¾æ€§é—å¿˜ã€å¤šä»»åŠ¡å­¦ä¹ ã€çŸ¥è¯†è¿ç§»
3. æ€§èƒ½ä¼˜åŒ–å’Œè°ƒè¯• - ä»£ç æ€§èƒ½åˆ†æã€å†…å­˜ä¼˜åŒ–ã€å¹¶è¡Œè®¡ç®—
4. å¤šç¯å¢ƒå…¼å®¹æ€§æµ‹è¯• - CPU/GPUã€æ“ä½œç³»ç»Ÿã€ä¾èµ–ç‰ˆæœ¬
"""

import os
import sys
import json
import time
import subprocess
import multiprocessing
import psutil
import platform
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime
import warnings
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import tempfile
import shutil

# å…¼å®¹æ€§æ£€æŸ¥
TORCH_AVAILABLE = False
CUDA_AVAILABLE = False
SKLEARN_AVAILABLE = False
MATPLOTLIB_AVAILABLE = False

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    TORCH_AVAILABLE = True
    CUDA_AVAILABLE = torch.cuda.is_available()
except ImportError:
    print("âš ï¸ è­¦å‘Š: PyTorchæœªå®‰è£…ï¼Œéƒ¨åˆ†æµ‹è¯•å°†å—é™")

try:
    from sklearn.metrics import accuracy_score, f1_score
    from sklearn.datasets import make_classification
    SKLEARN_AVAILABLE = True
except ImportError:
    print("âš ï¸ è­¦å‘Š: scikit-learnæœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å°†å—é™")

try:
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    print("âš ï¸ è­¦å‘Š: matplotlibæœªå®‰è£…ï¼Œå¯è§†åŒ–åŠŸèƒ½å°†å—é™")


class TestDatasetGenerator:
    """æ ‡å‡†åŒ–æµ‹è¯•æ•°æ®é›†ç”Ÿæˆå™¨"""
    
    @staticmethod
    def create_mnist_like_data(n_samples=1000, n_features=64, n_classes=10, noise=0.1):
        """åˆ›å»ºMNISTé£æ ¼æ•°æ®"""
        X, y = make_classification(
            n_samples=n_samples,
            n_features=n_features,
            n_informative=n_features // 2,
            n_redundant=n_features // 4,
            n_classes=n_classes,
            n_clusters_per_class=1,
            flip_y=noise,
            random_state=42
        )
        return X.astype(np.float32), y.astype(np.int32)
    
    @staticmethod
    def create_cifar_like_data(n_samples=1000, n_features=128, n_classes=10, noise=0.1):
        """åˆ›å»ºCIFARé£æ ¼æ•°æ®"""
        X, y = make_classification(
            n_samples=n_samples,
            n_features=n_features,
            n_informative=n_features // 3,
            n_redundant=n_features // 6,
            n_classes=n_classes,
            n_clusters_per_class=2,
            flip_y=noise,
            random_state=43
        )
        return X.astype(np.float32), y.astype(np.int32)
    
    @staticmethod
    def create_synthetic_data(n_samples=1000, n_features=64, n_classes=5, noise=0.05):
        """åˆ›å»ºåˆæˆæ•°æ®"""
        np.random.seed(42)
        X = np.random.randn(n_samples, n_features).astype(np.float32)
        centers = np.random.randn(n_classes, n_features)
        
        # åˆ†é…æ ·æœ¬åˆ°èšç±»
        y = np.random.choice(n_classes, n_samples)
        
        # æ·»åŠ èšç±»ç»“æ„
        for i in range(n_samples):
            X[i] = centers[y[i]] + 0.5 * np.random.randn(n_features)
        
        # æ·»åŠ å™ªå£°
        X += noise * np.random.randn(n_samples, n_features)
        
        return X.astype(np.float32), y.astype(np.int32)
    
    @staticmethod
    def create_continual_learning_data(n_tasks=5, n_samples_per_task=500, 
                                     n_features=32, n_classes_per_task=2):
        """åˆ›å»ºæŒç»­å­¦ä¹ æ•°æ®"""
        datasets = []
        
        for task_id in range(n_tasks):
            # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºä¸åŒçš„æ•°æ®åˆ†å¸ƒ
            np.random.seed(42 + task_id)
            
            X = np.random.randn(n_samples_per_task, n_features).astype(np.float32)
            
            # åˆ›å»ºä»»åŠ¡ç‰¹å®šçš„èšç±»ä¸­å¿ƒ
            centers = np.random.randn(n_classes_per_task, n_features)
            
            # åˆ†é…æ ·æœ¬
            y = np.random.choice(n_classes_per_task, n_samples_per_task)
            
            # æ·»åŠ èšç±»ç»“æ„
            for i in range(n_samples_per_task):
                X[i] = centers[y[i]] + 0.3 * np.random.randn(n_features)
                
            datasets.append((X.astype(np.float32), y.astype(np.int32)))
            
        return datasets


class BrainInspiredModel:
    """ç®€åŒ–çš„è„‘å¯å‘æ¨¡å‹ç”¨äºæµ‹è¯•"""
    
    def __init__(self, input_dim, hidden_dim, output_dim, use_attention=True):
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.use_attention = use_attention
        
        if TORCH_AVAILABLE:
            self.model = self._create_pytorch_model(input_dim, hidden_dim, output_dim, use_attention)
        else:
            self.model = self._create_numpy_model(input_dim, hidden_dim, output_dim)
    
    def _create_pytorch_model(self, input_dim, hidden_dim, output_dim, use_attention):
        """åˆ›å»ºPyTorchæ¨¡å‹"""
        class BrainInspiredModel(nn.Module):
            def __init__(self, input_dim, hidden_dim, output_dim, use_attention):
                super().__init__()
                self.use_attention = use_attention
                
                # ç¼–ç å™¨
                self.encoder = nn.Sequential(
                    nn.Linear(input_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(0.1),
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.ReLU()
                )
                
                # æ³¨æ„åŠ›æœºåˆ¶
                if use_attention:
                    self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8, batch_first=True)
                
                # åˆ†ç±»å™¨
                self.classifier = nn.Sequential(
                    nn.Linear(hidden_dim, hidden_dim // 2),
                    nn.ReLU(),
                    nn.Dropout(0.1),
                    nn.Linear(hidden_dim // 2, output_dim)
                )
            
            def forward(self, x):
                x = self.encoder(x)
                
                if self.use_attention:
                    x = x.unsqueeze(1)  # æ·»åŠ åºåˆ—ç»´åº¦
                    attended_x, _ = self.attention(x, x, x)
                    x = attended_x.squeeze(1)
                
                x = self.classifier(x)
                return x
        
        device = 'cuda' if CUDA_AVAILABLE else 'cpu'
        return BrainInspiredModel(input_dim, hidden_dim, output_dim, use_attention).to(device)
    
    def _create_numpy_model(self, input_dim, hidden_dim, output_dim):
        """åˆ›å»ºNumPyæ¨¡å‹ä½œä¸ºå¤‡é€‰"""
        class NumpyModel:
            def __init__(self, input_dim, hidden_dim, output_dim):
                # åˆå§‹åŒ–æƒé‡
                self.W1 = np.random.randn(input_dim, hidden_dim) * 0.1
                self.b1 = np.zeros(hidden_dim)
                self.W2 = np.random.randn(hidden_dim, hidden_dim) * 0.1
                self.b2 = np.zeros(hidden_dim)
                self.W3 = np.random.randn(hidden_dim, output_dim) * 0.1
                self.b3 = np.zeros(output_dim)
                
                # ç®€åŒ–æ³¨æ„åŠ›æƒé‡
                self.attention_weights = np.random.randn(hidden_dim, hidden_dim) * 0.1
            
            def forward(self, x):
                # å‰å‘ä¼ æ’­
                z1 = np.maximum(0, np.dot(x, self.W1) + self.b1)
                z2 = np.maximum(0, np.dot(z1, self.W2) + self.b2)
                
                # ç®€åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶
                attention_score = np.dot(z2, self.attention_weights)
                attended_z2 = z2 * np.tanh(attention_score)
                
                output = np.dot(attended_z2, self.W3) + self.b3
                return output
            
            def train(self, X, y, epochs=10, lr=0.001):
                # ç®€åŒ–è®­ç»ƒè¿‡ç¨‹
                for epoch in range(epochs):
                    # éšæœºæ¢¯åº¦ä¸‹é™
                    pass
        
        return NumpyModel(input_dim, hidden_dim, output_dim)
    
    def train(self, X, y, epochs=10, batch_size=32, lr=0.001):
        """è®­ç»ƒæ¨¡å‹"""
        if TORCH_AVAILABLE and isinstance(self.model, nn.Module):
            return self._train_pytorch(X, y, epochs, batch_size, lr)
        else:
            return self._train_numpy(X, y, epochs)
    
    def _train_pytorch(self, X, y, epochs, batch_size, lr):
        """PyTorchè®­ç»ƒ"""
        device = next(self.model.parameters()).device
        X_tensor = torch.FloatTensor(X).to(device)
        y_tensor = torch.LongTensor(y).to(device)
        
        dataset = TensorDataset(X_tensor, y_tensor)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        optimizer = optim.Adam(self.model.parameters(), lr=lr)
        criterion = nn.CrossEntropyLoss()
        
        self.model.train()
        losses = []
        
        for epoch in range(epochs):
            epoch_loss = 0
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            
            losses.append(epoch_loss / len(dataloader))
        
        return losses
    
    def _train_numpy(self, X, y, epochs):
        """NumPyè®­ç»ƒ"""
        self.model.train(X, y, epochs)
        return [0.0] * epochs
    
    def predict(self, X):
        """é¢„æµ‹"""
        if TORCH_AVAILABLE and isinstance(self.model, nn.Module):
            device = next(self.model.parameters()).device
            self.model.eval()
            with torch.no_grad():
                X_tensor = torch.FloatTensor(X).to(device)
                outputs = self.model(X_tensor)
                _, predicted = torch.max(outputs, 1)
                return predicted.cpu().numpy()
        else:
            outputs = self.model.forward(X)
            return np.argmax(outputs, axis=1)
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if TORCH_AVAILABLE and isinstance(self.model, nn.Module):
            device = next(self.model.parameters()).device
            self.model.eval()
            with torch.no_grad():
                X_tensor = torch.FloatTensor(X).to(device)
                outputs = self.model(X_tensor)
                return torch.softmax(outputs, dim=1).cpu().numpy()
        else:
            outputs = self.model.forward(X)
            exp_outputs = np.exp(outputs - np.max(outputs, axis=1, keepdims=True))
            return exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)


class BenchmarkTestSuite:
    """åŸºå‡†æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.dataset_generator = TestDatasetGenerator()
        self.results = {}
        self.test_config = {
            'input_dim': 64,
            'hidden_dim': 128,
            'output_dim': 10,
            'epochs': 10,
            'batch_size': 32,
            'learning_rate': 0.001
        }
    
    def run_training_speed_benchmark(self):
        """è®­ç»ƒé€Ÿåº¦åŸºå‡†æµ‹è¯•"""
        print("\nğŸš€ è®­ç»ƒé€Ÿåº¦åŸºå‡†æµ‹è¯•")
        print("=" * 60)
        
        test_models = ['brain_inspired_attention', 'brain_inspired_no_attention', 'standard']
        dataset_sizes = [500, 1000, 2000]
        
        results = {
            'config': self.test_config,
            'models_tested': test_models,
            'dataset_sizes': dataset_sizes,
            'results': {}
        }
        
        for model_name in test_models:
            print(f"\nğŸ—ï¸ æµ‹è¯•æ¨¡å‹: {model_name}")
            results['results'][model_name] = {}
            
            for size in dataset_sizes:
                print(f"   æ•°æ®é›†å¤§å°: {size}")
                
                # ç”Ÿæˆæ•°æ®
                X, y = self.dataset_generator.create_synthetic_data(size)
                
                # åˆ›å»ºæ¨¡å‹
                if model_name == 'brain_inspired_attention':
                    model = BrainInspiredModel(
                        X.shape[1], 
                        self.test_config['hidden_dim'],
                        len(np.unique(y)),
                        use_attention=True
                    )
                elif model_name == 'brain_inspired_no_attention':
                    model = BrainInspiredModel(
                        X.shape[1], 
                        self.test_config['hidden_dim'],
                        len(np.unique(y)),
                        use_attention=False
                    )
                else:  # standard
                    model = BrainInspiredModel(
                        X.shape[1], 
                        self.test_config['hidden_dim'],
                        len(np.unique(y)),
                        use_attention=False
                    )
                
                # æ€§èƒ½æµ‹è¯•
                start_time = time.time()
                start_memory = psutil.Process().memory_info().rss / 1024 / 1024
                
                try:
                    losses = model.train(
                        X, y,
                        epochs=self.test_config['epochs'],
                        batch_size=self.test_config['batch_size'],
                        lr=self.test_config['learning_rate']
                    )
                    
                    end_time = time.time()
                    end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    
                    training_time = end_time - start_time
                    memory_used = end_memory - start_memory
                    throughput = size / training_time if training_time > 0 else 0
                    
                    result = {
                        'training_time': training_time,
                        'memory_used': memory_used,
                        'throughput': throughput,
                        'final_loss': losses[-1] if losses else 0.0,
                        'success': True
                    }
                    
                    print(f"     è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
                    print(f"     ååé‡: {throughput:.1f} æ ·æœ¬/ç§’")
                    print(f"     å†…å­˜ä½¿ç”¨: {memory_used:.1f} MB")
                    
                except Exception as e:
                    result = {
                        'error': str(e),
                        'success': False
                    }
                    print(f"     âŒ è®­ç»ƒå¤±è´¥: {e}")
                
                results['results'][model_name][f'size_{size}'] = result
                
                # æ¸…ç†å†…å­˜
                del model, X, y
                
        return results
    
    def run_inference_speed_benchmark(self):
        """æ¨ç†é€Ÿåº¦åŸºå‡†æµ‹è¯•"""
        print("\nâš¡ æ¨ç†é€Ÿåº¦åŸºå‡†æµ‹è¯•")
        print("=" * 60)
        
        test_models = ['brain_inspired_attention', 'brain_inspired_no_attention']
        batch_sizes = [1, 16, 32, 64, 128]
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        X_test, _ = self.dataset_generator.create_synthetic_data(1000)
        
        results = {
            'config': self.test_config,
            'models_tested': test_models,
            'batch_sizes': batch_sizes,
            'test_samples': len(X_test),
            'results': {}
        }
        
        for model_name in test_models:
            print(f"\nğŸ—ï¸ æµ‹è¯•æ¨¡å‹: {model_name}")
            results['results'][model_name] = {}
            
            # åˆ›å»ºæ¨¡å‹
            model = BrainInspiredModel(
                X_test.shape[1],
                self.test_config['hidden_dim'],
                self.test_config['output_dim'],
                use_attention=(model_name == 'brain_inspired_attention')
            )
            
            # é¢„çƒ­
            warmup_X = X_test[:100]
            try:
                if TORCH_AVAILABLE:
                    model.model.eval()
                    with torch.no_grad():
                        warmup_tensor = torch.FloatTensor(warmup_X).to(
                            next(model.model.parameters()).device
                        )
                        for _ in range(10):
                            _ = model.model(warmup_tensor)
            except:
                pass
            
            for batch_size in batch_sizes:
                print(f"   æ‰¹å¤„ç†å¤§å°: {batch_size}")
                
                batch_X = X_test[:batch_size]
                
                start_time = time.time()
                start_memory = psutil.Process().memory_info().rss / 1024 / 1024
                
                try:
                    # è¿è¡Œ100æ¬¡æ¨ç†å–å¹³å‡
                    inference_times = []
                    for _ in range(100):
                        inference_start = time.time()
                        predictions = model.predict(batch_X)
                        inference_end = time.time()
                        inference_times.append(inference_end - inference_start)
                    
                    end_time = time.time()
                    end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    
                    avg_inference_time = np.mean(inference_times)
                    throughput = batch_size / avg_inference_time if avg_inference_time > 0 else 0
                    
                    result = {
                        'avg_inference_time': avg_inference_time,
                        'throughput': throughput,
                        'memory_used': end_memory - start_memory,
                        'success': True
                    }
                    
                    print(f"     å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time*1000:.2f}ms")
                    print(f"     ååé‡: {throughput:.1f} æ ·æœ¬/ç§’")
                    
                except Exception as e:
                    result = {
                        'error': str(e),
                        'success': False
                    }
                    print(f"     âŒ æ¨ç†å¤±è´¥: {e}")
                
                results['results'][model_name][f'batch_{batch_size}'] = result
        
        return results
    
    def run_memory_usage_benchmark(self):
        """å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•"""
        print("\nğŸ’¾ å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•")
        print("=" * 60)
        
        test_models = ['brain_inspired_attention', 'brain_inspired_no_attention']
        dataset_sizes = [500, 1000, 2000, 5000]
        
        results = {
            'config': self.test_config,
            'models_tested': test_models,
            'dataset_sizes': dataset_sizes,
            'results': {}
        }
        
        for model_name in test_models:
            print(f"\nğŸ—ï¸ æµ‹è¯•æ¨¡å‹: {model_name}")
            results['results'][model_name] = {}
            
            for size in dataset_sizes:
                print(f"   æ•°æ®é›†å¤§å°: {size}")
                
                # è®°å½•åˆå§‹å†…å­˜
                initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
                
                try:
                    # ç”Ÿæˆæ•°æ®
                    X, y = self.dataset_generator.create_synthetic_data(size)
                    after_data_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    data_memory = after_data_memory - initial_memory
                    
                    # åˆ›å»ºæ¨¡å‹
                    model = BrainInspiredModel(
                        X.shape[1],
                        self.test_config['hidden_dim'],
                        len(np.unique(y)),
                        use_attention=(model_name == 'brain_inspired_attention')
                    )
                    after_model_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    model_memory = after_model_memory - after_data_memory
                    
                    # è®­ç»ƒæ—¶å†…å­˜
                    model.train(X, y, epochs=2, batch_size=32)  # å‡å°‘è®­ç»ƒè½®æ•°ä»¥èŠ‚çœæ—¶é—´
                    peak_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    
                    result = {
                        'data_memory': data_memory,
                        'model_memory': model_memory,
                        'peak_memory': peak_memory - initial_memory,
                        'total_memory': peak_memory - initial_memory,
                        'success': True
                    }
                    
                    print(f"     æ•°æ®å†…å­˜: {data_memory:.1f} MB")
                    print(f"     æ¨¡å‹å†…å­˜: {model_memory:.1f} MB")
                    print(f"     å³°å€¼å†…å­˜: {result['peak_memory']:.1f} MB")
                    
                except Exception as e:
                    result = {
                        'error': str(e),
                        'success': False
                    }
                    print(f"     âŒ å†…å­˜æµ‹è¯•å¤±è´¥: {e}")
                
                results['results'][model_name][f'size_{size}'] = result
                
                # æ¸…ç†
                del model, X, y
                
        return results


class ContinualLearningTestSuite:
    """æŒç»­å­¦ä¹ èƒ½åŠ›æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.dataset_generator = TestDatasetGenerator()
    
    def test_catastrophic_forgetting(self):
        """æµ‹è¯•ç¾éš¾æ€§é—å¿˜"""
        print("\nğŸ§  ç¾éš¾æ€§é—å¿˜æµ‹è¯•")
        print("=" * 60)
        
        # ä»»åŠ¡1: å­¦ä¹ åˆå§‹ä»»åŠ¡
        print("ğŸ“š ä»»åŠ¡1: å­¦ä¹ åˆå§‹ä»»åŠ¡")
        task1_X, task1_y = self.dataset_generator.create_synthetic_data(
            n_samples=800, n_features=32, n_classes=5, noise=0.05
        )
        
        model = BrainInspiredModel(32, 64, 5, use_attention=True)
        model.train(task1_X, task1_y, epochs=15)
        
        # æµ‹è¯•åˆå§‹ä»»åŠ¡æ€§èƒ½
        task1_accuracy_before = self._evaluate_model(model, task1_X, task1_y)
        print(f"   ä»»åŠ¡1åˆå§‹å‡†ç¡®ç‡: {task1_accuracy_before:.4f}")
        
        # ä»»åŠ¡2: å­¦ä¹ æ–°ä»»åŠ¡
        print("ğŸ“š ä»»åŠ¡2: å­¦ä¹ æ–°ä»»åŠ¡")
        task2_X, task2_y = self.dataset_generator.create_synthetic_data(
            n_samples=800, n_features=32, n_classes=5, noise=0.05
        )
        
        model.train(task2_X, task2_y, epochs=15)
        
        # æµ‹è¯•ä¸¤ä¸ªä»»åŠ¡çš„æ€§èƒ½
        task1_accuracy_after = self._evaluate_model(model, task1_X, task1_y)
        task2_accuracy = self._evaluate_model(model, task2_X, task2_y)
        
        forgetting_rate = task1_accuracy_before - task1_accuracy_after
        
        results = {
            'task1_accuracy_before': task1_accuracy_before,
            'task1_accuracy_after': task1_accuracy_after,
            'task2_accuracy': task2_accuracy,
            'forgetting_rate': forgetting_rate,
            'success': True
        }
        
        print(f"   ä»»åŠ¡1ä¿æŒå‡†ç¡®ç‡: {task1_accuracy_after:.4f}")
        print(f"   ä»»åŠ¡2å‡†ç¡®ç‡: {task2_accuracy:.4f}")
        print(f"   é—å¿˜ç‡: {forgetting_rate:.4f}")
        
        return results
    
    def test_multitask_learning(self):
        """æµ‹è¯•å¤šä»»åŠ¡å­¦ä¹ """
        print("\nğŸ”„ å¤šä»»åŠ¡å­¦ä¹ éªŒè¯")
        print("=" * 60)
        
        # åˆ›å»ºå¤šä»»åŠ¡æ•°æ®
        task1_X, task1_y = self.dataset_generator.create_synthetic_data(
            n_samples=600, n_features=32, n_classes=3, noise=0.05
        )
        task2_X, task2_y = self.dataset_generator.create_synthetic_data(
            n_samples=600, n_features=32, n_classes=4, noise=0.05
        )
        task3_X, task3_y = self.dataset_generator.create_synthetic_data(
            n_samples=600, n_features=32, n_classes=2, noise=0.05
        )
        
        # åˆ›å»ºå¤šä»»åŠ¡æ¨¡å‹
        model = BrainInspiredModel(32, 64, 9, use_attention=True)  # 3+4+2=9 classes
        
        # è®­ç»ƒå¤šä»»åŠ¡
        combined_X = np.vstack([task1_X, task2_X, task3_X])
        combined_y = np.hstack([task1_y, task2_y + 3, task3_y + 7])  # è°ƒæ•´æ ‡ç­¾
        
        model.train(combined_X, combined_y, epochs=20)
        
        # è¯„ä¼°å„ä»»åŠ¡æ€§èƒ½
        task1_acc = self._evaluate_classification(model, task1_X, task1_y, range(3))
        task2_acc = self._evaluate_classification(model, task2_X, task2_y, range(3, 7), offset=-3)
        task3_acc = self._evaluate_classification(model, task3_X, task3_y, range(7, 9), offset=-7)
        
        results = {
            'task1_accuracy': task1_acc,
            'task2_accuracy': task2_acc,
            'task3_accuracy': task3_acc,
            'average_accuracy': (task1_acc + task2_acc + task3_acc) / 3,
            'success': True
        }
        
        print(f"   ä»»åŠ¡1å‡†ç¡®ç‡: {task1_acc:.4f}")
        print(f"   ä»»åŠ¡2å‡†ç¡®ç‡: {task2_acc:.4f}")
        print(f"   ä»»åŠ¡3å‡†ç¡®ç‡: {task3_acc:.4f}")
        print(f"   å¹³å‡å‡†ç¡®ç‡: {results['average_accuracy']:.4f}")
        
        return results
    
    def test_knowledge_transfer(self):
        """æµ‹è¯•çŸ¥è¯†è¿ç§»"""
        print("\nğŸ¯ çŸ¥è¯†è¿ç§»æµ‹è¯•")
        print("=" * 60)
        
        # æºä»»åŠ¡ï¼šå­¦ä¹ ä¸€èˆ¬ç‰¹å¾
        print("ğŸ“š æºä»»åŠ¡: å­¦ä¹ ä¸€èˆ¬ç‰¹å¾")
        source_X, source_y = self.dataset_generator.create_synthetic_data(
            n_samples=1000, n_features=64, n_classes=5, noise=0.05
        )
        
        # åˆ›å»ºé¢„è®­ç»ƒæ¨¡å‹
        pretrained_model = BrainInspiredModel(64, 128, 5, use_attention=True)
        pretrained_model.train(source_X, source_y, epochs=25)
        
        # æºä»»åŠ¡æ€§èƒ½
        source_accuracy = self._evaluate_model(pretrained_model, source_X, source_y)
        print(f"   æºä»»åŠ¡å‡†ç¡®ç‡: {source_accuracy:.4f}")
        
        # ç›®æ ‡ä»»åŠ¡ï¼šå­¦ä¹ ç‰¹å®šç‰¹å¾ï¼ˆç›¸ä¼¼çš„åˆ†å¸ƒï¼‰
        print("ğŸ“š ç›®æ ‡ä»»åŠ¡: çŸ¥è¯†è¿ç§»")
        target_X, target_y = self.dataset_generator.create_synthetic_data(
            n_samples=800, n_features=64, n_classes=5, noise=0.05
        )
        
        # è¿ç§»å­¦ä¹ ï¼šå†»ç»“ç¼–ç å™¨ï¼Œåªè®­ç»ƒåˆ†ç±»å™¨
        frozen_model = BrainInspiredModel(64, 128, 5, use_attention=True)
        
        # å¤åˆ¶é¢„è®­ç»ƒæƒé‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        if TORCH_AVAILABLE and hasattr(pretrained_model.model, 'encoder'):
            # å¤åˆ¶ç¼–ç å™¨æƒé‡
            if hasattr(frozen_model.model, 'encoder'):
                try:
                    frozen_model.model.encoder.load_state_dict(pretrained_model.model.encoder.state_dict())
                    print("   ğŸ”’ ç¼–ç å™¨å·²å†»ç»“ï¼Œä½¿ç”¨é¢„è®­ç»ƒæƒé‡")
                except:
                    print("   âš ï¸ æ— æ³•å†»ç»“ç¼–ç å™¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
        else:
            print("   âš ï¸ ç®€åŒ–ç‰ˆæœ¬ï¼Œè·³è¿‡æƒé‡å†»ç»“")
        
        # å¿«é€Ÿå¾®è°ƒ
        frozen_model.train(target_X, target_y, epochs=5)
        
        # è¯„ä¼°è¿ç§»æ€§èƒ½
        transfer_accuracy = self._evaluate_model(frozen_model, target_X, target_y)
        
        # æ¯”è¾ƒä»å¤´è®­ç»ƒçš„æ€§èƒ½
        scratch_model = BrainInspiredModel(64, 128, 5, use_attention=True)
        scratch_model.train(target_X, target_y, epochs=5)
        scratch_accuracy = self._evaluate_model(scratch_model, target_X, target_y)
        
        transfer_advantage = transfer_accuracy - scratch_accuracy
        
        results = {
            'source_accuracy': source_accuracy,
            'transfer_accuracy': transfer_accuracy,
            'scratch_accuracy': scratch_accuracy,
            'transfer_advantage': transfer_advantage,
            'success': True
        }
        
        print(f"   è¿ç§»å­¦ä¹ å‡†ç¡®ç‡: {transfer_accuracy:.4f}")
        print(f"   ä»å¤´è®­ç»ƒå‡†ç¡®ç‡: {scratch_accuracy:.4f}")
        print(f"   è¿ç§»ä¼˜åŠ¿: {transfer_advantage:.4f}")
        
        return results
    
    def _evaluate_model(self, model, X, y):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        predictions = model.predict(X)
        accuracy = np.mean(predictions == y)
        return accuracy
    
    def _evaluate_classification(self, model, X, y, class_indices, offset=0):
        """è¯„ä¼°ç‰¹å®šç±»åˆ«åˆ†ç±»"""
        predictions = model.predict(X)
        
        # åªè€ƒè™‘æŒ‡å®šç±»åˆ«
        mask = np.isin(y + offset, class_indices)
        if not np.any(mask):
            return 0.0
        
        correct = np.sum((predictions[mask] == (y[mask] + offset)))
        total = np.sum(mask)
        
        return correct / total if total > 0 else 0.0


class PerformanceOptimizationSuite:
    """æ€§èƒ½ä¼˜åŒ–å’Œè°ƒè¯•å¥—ä»¶"""
    
    def __init__(self):
        self.dataset_generator = TestDatasetGenerator()
    
    def profile_code_performance(self):
        """ä»£ç æ€§èƒ½åˆ†æ"""
        print("\nğŸ“Š ä»£ç æ€§èƒ½åˆ†æ")
        print("=" * 60)
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        X, y = self.dataset_generator.create_synthetic_data(2000, 64, 10, 0.05)
        
        # æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½
        configs = [
            {'use_attention': True, 'hidden_dim': 64, 'batch_size': 32},
            {'use_attention': True, 'hidden_dim': 128, 'batch_size': 32},
            {'use_attention': True, 'hidden_dim': 128, 'batch_size': 64},
            {'use_attention': False, 'hidden_dim': 128, 'batch_size': 32},
        ]
        
        results = {
            'test_data_size': len(X),
            'configurations': configs,
            'results': []
        }
        
        for i, config in enumerate(configs):
            print(f"\nğŸ—ï¸ é…ç½® {i+1}: {config}")
            
            try:
                # åˆ›å»ºæ¨¡å‹
                model = BrainInspiredModel(
                    X.shape[1],
                    config['hidden_dim'],
                    len(np.unique(y)),
                    use_attention=config['use_attention']
                )
                
                # æ€§èƒ½æµ‹è¯•
                start_time = time.time()
                start_memory = psutil.Process().memory_info().rss / 1024 / 1024
                
                # è®­ç»ƒ
                losses = model.train(
                    X, y,
                    epochs=10,
                    batch_size=config['batch_size']
                )
                
                # æ¨ç†æµ‹è¯•
                inference_times = []
                for _ in range(100):
                    inference_start = time.time()
                    predictions = model.predict(X[:100])
                    inference_end = time.time()
                    inference_times.append(inference_end - inference_start)
                
                end_time = time.time()
                end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                
                # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                training_time = end_time - start_time
                memory_used = end_memory - start_memory
                avg_inference_time = np.mean(inference_times)
                
                # è®¡ç®—æœ€ç»ˆå‡†ç¡®ç‡
                final_accuracy = self._evaluate_model(model, X[:500], y[:500])
                
                config_result = {
                    'config': config,
                    'training_time': training_time,
                    'memory_used': memory_used,
                    'avg_inference_time': avg_inference_time,
                    'final_accuracy': final_accuracy,
                    'throughput': len(X) / training_time,
                    'success': True
                }
                
                print(f"   è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
                print(f"   å†…å­˜ä½¿ç”¨: {memory_used:.1f} MB")
                print(f"   æ¨ç†æ—¶é—´: {avg_inference_time*1000:.2f}ms")
                print(f"   æœ€ç»ˆå‡†ç¡®ç‡: {final_accuracy:.4f}")
                
            except Exception as e:
                config_result = {
                    'config': config,
                    'error': str(e),
                    'success': False
                }
                print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
            
            results['results'].append(config_result)
            del model
        
        return results
    
    def optimize_memory_usage(self):
        """å†…å­˜ä¼˜åŒ–æµ‹è¯•"""
        print("\nğŸ’¾ å†…å­˜ä¼˜åŒ–æµ‹è¯•")
        print("=" * 60)
        
        # æµ‹è¯•ä¸åŒæ‰¹é‡å¤§å°çš„å†…å­˜ä½¿ç”¨
        batch_sizes = [16, 32, 64, 128, 256]
        
        X, y = self.dataset_generator.create_synthetic_data(5000, 64, 10, 0.05)
        
        results = {
            'test_data_size': len(X),
            'batch_sizes': batch_sizes,
            'results': {}
        }
        
        for batch_size in batch_sizes:
            print(f"\nğŸ“¦ æ‰¹é‡å¤§å°: {batch_size}")
            
            try:
                # è®°å½•åˆå§‹å†…å­˜
                initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
                
                # åˆ›å»ºæ¨¡å‹
                model = BrainInspiredModel(64, 128, 10, use_attention=True)
                
                # è®­ç»ƒå¹¶ç›‘æ§å†…å­˜
                model.train(X, y, epochs=5, batch_size=batch_size)
                
                peak_memory = psutil.Process().memory_info().rss / 1024 / 1024
                memory_used = peak_memory - initial_memory
                
                # æµ‹è¯•æ¨ç†æ—¶çš„å†…å­˜
                inference_start = psutil.Process().memory_info().rss / 1024 / 1024
                predictions = model.predict(X[:1000])
                inference_end = psutil.Process().memory_info().rss / 1024 / 1024
                inference_memory = inference_end - inference_start
                
                batch_result = {
                    'training_memory': memory_used,
                    'inference_memory_increase': inference_memory,
                    'total_memory': memory_used + inference_memory,
                    'success': True
                }
                
                print(f"   è®­ç»ƒå†…å­˜: {memory_used:.1f} MB")
                print(f"   æ¨ç†å†…å­˜å¢é‡: {inference_memory:.1f} MB")
                
            except Exception as e:
                batch_result = {
                    'error': str(e),
                    'success': False
                }
                print(f"   âŒ å†…å­˜æµ‹è¯•å¤±è´¥: {e}")
            
            results['results'][f'batch_{batch_size}'] = batch_result
            del model
        
        return results
    
    def test_parallel_processing(self):
        """å¹¶è¡Œè®¡ç®—ä¼˜åŒ–æµ‹è¯•"""
        print("\nâš¡ å¹¶è¡Œè®¡ç®—ä¼˜åŒ–æµ‹è¯•")
        print("=" * 60)
        
        # æµ‹è¯•å•çº¿ç¨‹vså¤šçº¿ç¨‹æ€§èƒ½
        num_workers_list = [1, 2, 4, 8]
        dataset_sizes = [1000, 2000, 4000]
        
        results = {
            'test_configurations': [],
            'results': {}
        }
        
        for dataset_size in dataset_sizes:
            print(f"\nğŸ“Š æ•°æ®é›†å¤§å°: {dataset_size}")
            
            # ç”Ÿæˆæ•°æ®
            X, y = self.dataset_generator.create_synthetic_data(
                dataset_size, 64, 10, 0.05
            )
            
            results['results'][f'size_{dataset_size}'] = {}
            
            for num_workers in num_workers_list:
                print(f"   ğŸ”§ å·¥ä½œè¿›ç¨‹æ•°: {num_workers}")
                
                try:
                    start_time = time.time()
                    
                    # ç®€åŒ–çš„å¹¶è¡Œå¤„ç†æµ‹è¯•
                    if TORCH_AVAILABLE and num_workers > 1:
                        # ä½¿ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½å™¨
                        model = BrainInspiredModel(64, 128, 10, use_attention=True)
                        
                        dataset = TensorDataset(
                            torch.FloatTensor(X),
                            torch.LongTensor(y)
                        )
                        
                        dataloader = DataLoader(
                            dataset,
                            batch_size=32,
                            shuffle=True,
                            num_workers=num_workers
                        )
                        
                        # è®­ç»ƒ
                        optimizer = optim.Adam(model.model.parameters(), lr=0.001)
                        criterion = nn.CrossEntropyLoss()
                        
                        model.model.train()
                        for epoch in range(3):
                            for batch_X, batch_y in dataloader:
                                optimizer.zero_grad()
                                outputs = model.model(batch_X)
                                loss = criterion(outputs, batch_y)
                                loss.backward()
                                optimizer.step()
                    else:
                        # å•è¿›ç¨‹ç‰ˆæœ¬
                        model = BrainInspiredModel(64, 128, 10, use_attention=True)
                        model.train(X, y, epochs=3, batch_size=32)
                    
                    end_time = time.time()
                    processing_time = end_time - start_time
                    
                    parallel_result = {
                        'processing_time': processing_time,
                        'num_workers': num_workers,
                        'success': True
                    }
                    
                    print(f"     å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
                    
                except Exception as e:
                    parallel_result = {
                        'error': str(e),
                        'num_workers': num_workers,
                        'success': False
                    }
                    print(f"     âŒ å¹¶è¡Œæµ‹è¯•å¤±è´¥: {e}")
                
                results['results'][f'size_{dataset_size}'][f'workers_{num_workers}'] = parallel_result
                del model
        
        return results
    
    def _evaluate_model(self, model, X, y):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        predictions = model.predict(X)
        accuracy = np.mean(predictions == y)
        return accuracy


class CompatibilityTestSuite:
    """å¤šç¯å¢ƒå…¼å®¹æ€§æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.system_info = self._collect_system_info()
    
    def test_cpu_gpu_compatibility(self):
        """æµ‹è¯•CPU/GPUå…¼å®¹æ€§"""
        print("\nğŸ–¥ï¸ CPU/GPUå…¼å®¹æ€§æµ‹è¯•")
        print("=" * 60)
        
        results = {
            'system_info': self.system_info,
            'device_tests': {}
        }
        
        # æµ‹è¯•CPUç‰ˆæœ¬
        print("ğŸ”§ æµ‹è¯•CPUç‰ˆæœ¬")
        cpu_result = self._test_device('cpu')
        results['device_tests']['cpu'] = cpu_result
        
        # æµ‹è¯•GPUç‰ˆæœ¬ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if CUDA_AVAILABLE:
            print("ğŸ”§ æµ‹è¯•GPUç‰ˆæœ¬")
            gpu_result = self._test_device('cuda')
            results['device_tests']['gpu'] = gpu_result
        else:
            print("âš ï¸ GPUä¸å¯ç”¨ï¼Œè·³è¿‡GPUæµ‹è¯•")
            results['device_tests']['gpu'] = {'available': False}
        
        return results
    
    def test_operating_system_compatibility(self):
        """æµ‹è¯•æ“ä½œç³»ç»Ÿå…¼å®¹æ€§"""
        print("\nğŸ–¥ï¸ æ“ä½œç³»ç»Ÿå…¼å®¹æ€§æµ‹è¯•")
        print("=" * 60)
        
        os_info = {
            'platform': platform.system(),
            'platform_release': platform.release(),
            'architecture': platform.machine(),
            'python_version': platform.python_version()
        }
        
        results = {
            'os_info': os_info,
            'compatibility_tests': {}
        }
        
        # åŸºç¡€åŠŸèƒ½æµ‹è¯•
        print("ğŸ”§ åŸºç¡€åŠŸèƒ½æµ‹è¯•")
        try:
            # æµ‹è¯•æ–‡ä»¶æ“ä½œ
            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as f:
                f.write("æµ‹è¯•æ–‡ä»¶")
                temp_file = f.name
            
            with open(temp_file, 'r') as f:
                content = f.read()
            
            os.remove(temp_file)
            file_operations = {'success': True, 'result': 'æ­£å¸¸'}
            
        except Exception as e:
            file_operations = {'success': False, 'error': str(e)}
        
        # æµ‹è¯•è·¯å¾„å¤„ç†
        try:
            test_path = Path('/tmp/test_path_12345')
            test_path.mkdir(exist_ok=True)
            test_path.rmdir()
            path_operations = {'success': True, 'result': 'æ­£å¸¸'}
        except Exception as e:
            path_operations = {'success': False, 'error': str(e)}
        
        # æµ‹è¯•ç¯å¢ƒå˜é‡
        try:
            import os
            test_env_var = f"TEST_VAR_{int(time.time())}"
            os.environ[test_env_var] = "test_value"
            retrieved_value = os.environ.get(test_env_var, "")
            del os.environ[test_env_var]
            env_operations = {'success': True, 'result': 'æ­£å¸¸'} if retrieved_value == "test_value" else {'success': False, 'error': 'ç¯å¢ƒå˜é‡è¯»å†™å¤±è´¥'}
        except Exception as e:
            env_operations = {'success': False, 'error': str(e)}
        
        results['compatibility_tests'] = {
            'file_operations': file_operations,
            'path_operations': path_operations,
            'environment_variables': env_operations
        }
        
        print(f"   æ–‡ä»¶æ“ä½œ: {'âœ… æ­£å¸¸' if file_operations['success'] else 'âŒ å¤±è´¥'}")
        print(f"   è·¯å¾„æ“ä½œ: {'âœ… æ­£å¸¸' if path_operations['success'] else 'âŒ å¤±è´¥'}")
        print(f"   ç¯å¢ƒå˜é‡: {'âœ… æ­£å¸¸' if env_operations['success'] else 'âŒ å¤±è´¥'}")
        
        return results
    
    def test_dependency_compatibility(self):
        """æµ‹è¯•ä¾èµ–ç‰ˆæœ¬å…¼å®¹æ€§"""
        print("\nğŸ“¦ ä¾èµ–ç‰ˆæœ¬å…¼å®¹æ€§æµ‹è¯•")
        print("=" * 60)
        
        dependencies = {
            'python': platform.python_version(),
            'torch': torch.__version__ if TORCH_AVAILABLE else 'Not Available',
            'sklearn': 'Available' if SKLEARN_AVAILABLE else 'Not Available',
            'matplotlib': 'Available' if MATPLOTLIB_AVAILABLE else 'Not Available',
            'numpy': np.__version__
        }
        
        # æµ‹è¯•ä¾èµ–å¯¼å…¥
        import_tests = {}
        
        # æµ‹è¯•NumPy
        try:
            import numpy as np
            x = np.array([1, 2, 3])
            result = x.sum()
            import_tests['numpy'] = {'success': True, 'version': np.__version__}
        except Exception as e:
            import_tests['numpy'] = {'success': False, 'error': str(e)}
        
        # æµ‹è¯•PyTorch
        if TORCH_AVAILABLE:
            try:
                import torch
                x = torch.tensor([1, 2, 3])
                result = x.sum().item()
                import_tests['torch'] = {'success': True, 'version': torch.__version__}
            except Exception as e:
                import_tests['torch'] = {'success': False, 'error': str(e)}
        else:
            import_tests['torch'] = {'success': False, 'error': 'PyTorchæœªå®‰è£…'}
        
        # æµ‹è¯•sklearn
        if SKLEARN_AVAILABLE:
            try:
                import sklearn
                import_tests['sklearn'] = {'success': True, 'version': sklearn.__version__}
            except Exception as e:
                import_tests['sklearn'] = {'success': False, 'error': str(e)}
        else:
            import_tests['sklearn'] = {'success': False, 'error': 'scikit-learnæœªå®‰è£…'}
        
        results = {
            'dependencies': dependencies,
            'import_tests': import_tests
        }
        
        print("ğŸ“‹ ä¾èµ–ç‰ˆæœ¬æ£€æŸ¥:")
        for dep, test_result in import_tests.items():
            status = "âœ…" if test_result['success'] else "âŒ"
            version = test_result.get('version', 'Unknown')
            print(f"   {dep}: {status} {version}")
        
        return results
    
    def _test_device(self, device):
        """æµ‹è¯•ç‰¹å®šè®¾å¤‡"""
        try:
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            X, y = TestDatasetGenerator().create_synthetic_data(1000, 64, 10, 0.05)
            
            # åˆ›å»ºæ¨¡å‹
            model = BrainInspiredModel(64, 128, 10, use_attention=True)
            
            # è®­ç»ƒæµ‹è¯•
            start_time = time.time()
            model.train(X, y, epochs=5, batch_size=32)
            end_time = time.time()
            
            # æ¨ç†æµ‹è¯•
            predictions = model.predict(X[:100])
            accuracy = np.mean(predictions == y[:100])
            
            return {
                'available': True,
                'training_time': end_time - start_time,
                'inference_accuracy': accuracy,
                'device': device,
                'success': True
            }
            
        except Exception as e:
            return {
                'available': False,
                'error': str(e),
                'device': device,
                'success': False
            }
    
    def _collect_system_info(self):
        """æ”¶é›†ç³»ç»Ÿä¿¡æ¯"""
        info = {
            'platform': platform.system(),
            'platform_release': platform.release(),
            'architecture': platform.machine(),
            'processor': platform.processor(),
            'python_version': platform.python_version(),
            'cpu_count': psutil.cpu_count(),
            'memory_total': psutil.virtual_memory().total,
            'memory_available': psutil.virtual_memory().available
        }
        
        if TORCH_AVAILABLE:
            info['torch_version'] = torch.__version__
            info['cuda_available'] = CUDA_AVAILABLE
            if CUDA_AVAILABLE:
                info['cuda_version'] = torch.version.cuda
                info['gpu_count'] = torch.cuda.device_count()
                for i in range(torch.cuda.device_count()):
                    info[f'gpu_{i}_name'] = torch.cuda.get_device_name(i)
                    info[f'gpu_{i}_memory'] = torch.cuda.get_device_properties(i).total_memory
        
        return info


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§  è„‘å¯å‘AIç³»ç»Ÿ - å…¨é¢æµ‹è¯•éªŒè¯å¥—ä»¶")
    print("=" * 80)
    print("æ—¶é—´:", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('data/results', exist_ok=True)
    
    # åˆå§‹åŒ–æµ‹è¯•å¥—ä»¶
    benchmark_suite = BenchmarkTestSuite()
    continual_suite = ContinualLearningTestSuite()
    performance_suite = PerformanceOptimizationSuite()
    compatibility_suite = CompatibilityTestSuite()
    
    # å­˜å‚¨æ‰€æœ‰æµ‹è¯•ç»“æœ
    all_results = {
        'timestamp': datetime.now().isoformat(),
        'system_info': compatibility_suite.system_info,
        'test_suites': {}
    }
    
    # 1. è¿è¡ŒåŸºå‡†æµ‹è¯•å¥—ä»¶
    print("\n" + "="*80)
    print("ğŸ“Š åŸºå‡†æµ‹è¯•å¥—ä»¶")
    print("="*80)
    
    benchmark_results = {}
    try:
        benchmark_results['training_speed'] = benchmark_suite.run_training_speed_benchmark()
    except Exception as e:
        benchmark_results['training_speed'] = {'error': str(e), 'success': False}
    
    try:
        benchmark_results['inference_speed'] = benchmark_suite.run_inference_speed_benchmark()
    except Exception as e:
        benchmark_results['inference_speed'] = {'error': str(e), 'success': False}
    
    try:
        benchmark_results['memory_usage'] = benchmark_suite.run_memory_usage_benchmark()
    except Exception as e:
        benchmark_results['memory_usage'] = {'error': str(e), 'success': False}
    
    all_results['test_suites']['benchmark'] = benchmark_results
    
    # 2. è¿è¡ŒæŒç»­å­¦ä¹ æµ‹è¯•å¥—ä»¶
    print("\n" + "="*80)
    print("ğŸ§  æŒç»­å­¦ä¹ æµ‹è¯•å¥—ä»¶")
    print("="*80)
    
    continual_results = {}
    try:
        continual_results['catastrophic_forgetting'] = continual_suite.test_catastrophic_forgetting()
    except Exception as e:
        continual_results['catastrophic_forgetting'] = {'error': str(e), 'success': False}
    
    try:
        continual_results['multitask_learning'] = continual_suite.test_multitask_learning()
    except Exception as e:
        continual_results['multitask_learning'] = {'error': str(e), 'success': False}
    
    try:
        continual_results['knowledge_transfer'] = continual_suite.test_knowledge_transfer()
    except Exception as e:
        continual_results['knowledge_transfer'] = {'error': str(e), 'success': False}
    
    all_results['test_suites']['continual_learning'] = continual_results
    
    # 3. è¿è¡Œæ€§èƒ½ä¼˜åŒ–æµ‹è¯•å¥—ä»¶
    print("\n" + "="*80)
    print("âš¡ æ€§èƒ½ä¼˜åŒ–æµ‹è¯•å¥—ä»¶")
    print("="*80)
    
    performance_results = {}
    try:
        performance_results['code_profiling'] = performance_suite.profile_code_performance()
    except Exception as e:
        performance_results['code_profiling'] = {'error': str(e), 'success': False}
    
    try:
        performance_results['memory_optimization'] = performance_suite.optimize_memory_usage()
    except Exception as e:
        performance_results['memory_optimization'] = {'error': str(e), 'success': False}
    
    try:
        performance_results['parallel_processing'] = performance_suite.test_parallel_processing()
    except Exception as e:
        performance_results['parallel_processing'] = {'error': str(e), 'success': False}
    
    all_results['test_suites']['performance'] = performance_results
    
    # 4. è¿è¡Œå…¼å®¹æ€§æµ‹è¯•å¥—ä»¶
    print("\n" + "="*80)
    print("ğŸ–¥ï¸ å…¼å®¹æ€§æµ‹è¯•å¥—ä»¶")
    print("="*80)
    
    compatibility_results = {}
    try:
        compatibility_results['cpu_gpu_compatibility'] = compatibility_suite.test_cpu_gpu_compatibility()
    except Exception as e:
        compatibility_results['cpu_gpu_compatibility'] = {'error': str(e), 'success': False}
    
    try:
        compatibility_results['os_compatibility'] = compatibility_suite.test_operating_system_compatibility()
    except Exception as e:
        compatibility_results['os_compatibility'] = {'error': str(e), 'success': False}
    
    try:
        compatibility_results['dependency_compatibility'] = compatibility_suite.test_dependency_compatibility()
    except Exception as e:
        compatibility_results['dependency_compatibility'] = {'error': str(e), 'success': False}
    
    all_results['test_suites']['compatibility'] = compatibility_results
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print("\n" + "="*80)
    print("ğŸ“ˆ ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
    print("="*80)
    
    # è®¡ç®—æ€»ä½“æˆåŠŸç‡
    total_tests = 0
    successful_tests = 0
    
    for suite_name, suite_results in all_results['test_suites'].items():
        for test_name, test_result in suite_results.items():
            total_tests += 1
            if isinstance(test_result, dict) and test_result.get('success', False):
                successful_tests += 1
            elif isinstance(test_result, dict) and 'success' not in test_result:
                # å¯¹äºåŒ…å«å¤šä¸ªå­æµ‹è¯•çš„ç»“æœ
                for sub_test in test_result.values():
                    total_tests += 1
                    if isinstance(sub_test, dict) and sub_test.get('success', False):
                        successful_tests += 1
    
    success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0
    
    summary = {
        'total_tests': total_tests,
        'successful_tests': successful_tests,
        'success_rate': success_rate,
        'timestamp': datetime.now().isoformat()
    }
    
    all_results['summary'] = summary
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f'data/results/comprehensive_test_results_{timestamp}.json'
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    # æ‰“å°æ€»ç»“
    print(f"\nğŸ‰ å…¨é¢æµ‹è¯•éªŒè¯å®Œæˆ!")
    print(f"ğŸ“Š æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"âœ… æˆåŠŸæµ‹è¯•æ•°: {successful_tests}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    if success_rate >= 90:
        print("ğŸ¯ ç³»ç»Ÿè´¨é‡è¯„çº§: ä¼˜ç§€")
    elif success_rate >= 75:
        print("ğŸ¯ ç³»ç»Ÿè´¨é‡è¯„çº§: è‰¯å¥½")
    elif success_rate >= 60:
        print("ğŸ¯ ç³»ç»Ÿè´¨é‡è¯„çº§: åˆæ ¼")
    else:
        print("ğŸ¯ ç³»ç»Ÿè´¨é‡è¯„çº§: éœ€è¦æ”¹è¿›")
    
    return success_rate


if __name__ == "__main__":
    success_rate = main()
    sys.exit(0 if success_rate >= 75 else 1)  # æˆåŠŸé€€å‡ºç ä¸º75%ä»¥ä¸ŠæˆåŠŸç‡